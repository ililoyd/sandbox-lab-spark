{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CONFIGURATION ZONE (MANDATORY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#SETUP\n",
    "Dir_X = \"newDATA/X/\"\n",
    "Output_Dir_X = \"newDATA/X_output/\"\n",
    "\n",
    "Dir_Y = \"newDATA/Y/\"\n",
    "Output_Dir_Y = \"newDATA/Y_output/\"\n",
    "\n",
    "Dir_Model = \"src/X_output/\"\n",
    "Multithread_gran = 6\n",
    "\n",
    "Export_name = \"merged_forHDFS2\"\n",
    "\n",
    "HDFS_Dir = \"/user/cluster/AdLearn/Debug/17-05-18/\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate Model Reference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "########## SCAN AND CONSTRUCT MATRIX FORM ######################\n",
    "filesList = glob.glob(Dir_X + \"*.csv\")\n",
    "\n",
    "ix = []\n",
    "col = []\n",
    "indexList = np.array(ix)\n",
    "colList = np.array(col)\n",
    "\n",
    "for f_ in filesList:\n",
    "    print(f_)\n",
    "    df = pd.read_csv(f_, index_col=0)\n",
    "    indexList = np.append(indexList, df.index.values)\n",
    "    indexList = np.unique(indexList)\n",
    "    indexList = np.sort(indexList)    \n",
    "    \n",
    "    colList = np.append(colList, df.columns.values)\n",
    "    colList = np.unique(colList)\n",
    "    colList = np.sort(colList)\n",
    "    \n",
    "\n",
    "## Generate the DF for double index stacking\n",
    "df = pd.DataFrame(index=indexList,columns=colList, data=0)\n",
    "temp = df.stack()\n",
    "\n",
    "## To drop the unwanted 0 column\n",
    "temp.to_csv(Dir_Model + \" _model.csv\")\n",
    "df = pd.read_csv(Dir_Model + \"_model.csv\")\n",
    "\n",
    "T = df.drop(\"0\", axis=1)\n",
    "\n",
    "#FINAL MODEL\n",
    "T.to_csv(Dir_Model + \"model.csv\", index=False)   \n",
    "os.remove(Dir_Model + \"_model.csv\")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare csv of X-type variables (Multithread)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import multiprocessing\n",
    "import time\n",
    "import glob\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import gc\n",
    "\n",
    "n_cores = multiprocessing.cpu_count()\n",
    "filesList = glob.glob(Dir_X + \"*.csv\")\n",
    "data = filesList\n",
    "\n",
    "def mp_worker(file_):\n",
    "    process = multiprocessing.current_process()\n",
    "    print \"File: \" + str(file_) \n",
    "    \n",
    "    \n",
    "    f_c = file_.replace('.csv','')\n",
    "    f_c = f_c.replace(Dir_X,'')\n",
    "    f_c_ = ''.join(e for e in f_c if e.isalnum())\n",
    "\n",
    "\n",
    "    test = pd.read_csv(file_, index_col=0)\n",
    "    \n",
    "    #@TODO: Rustine to BE DROPPED once fixed in data source\n",
    "    if 'CIQID' in test.columns:\n",
    "        del test['CIQID']\n",
    "    \n",
    "    #test = test.fillna(-1)\n",
    "    test = test.stack(dropna=False).to_frame()\n",
    "\n",
    "    model = pd.read_csv(Dir_Model + \"model.csv\", index_col=[0,1])\n",
    "\n",
    "    df = pd.DataFrame(index=model.index.difference(test.index),data=np.nan,columns=[0])\n",
    "    \n",
    "    frames = [test, df]\n",
    "    bDF = pd.concat(frames)\n",
    "    bDF.sort_index()\n",
    "    bDF.columns = [f_c_]\n",
    "\n",
    "    #bDF.to_csv(\"X_output/standalone/\"+f_c_+\".csv\",index=True)\n",
    "    bDF.to_csv(Output_Dir_X + f_c_ + \".csv\",index=False) \n",
    "    \n",
    "    del df\n",
    "    del bDF\n",
    "    del model\n",
    "    del test\n",
    "    gc.collect()\n",
    "    \n",
    "p = multiprocessing.Pool(Multithread_gran)\n",
    "p.map(mp_worker, data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import multiprocessing\n",
    "import time\n",
    "import glob\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import gc\n",
    "from functools import partial\n",
    "\n",
    "def mp_worker(file_, modelDir, workingDir, outputDir):\n",
    "    process = multiprocessing.current_process()\n",
    "    print \"File: \" + str(file_) \n",
    "    \n",
    "    \n",
    "    f_c = file_.replace('.csv','')\n",
    "    f_c = f_c.replace(workingDir,'')\n",
    "    f_c_ = ''.join(e for e in f_c if e.isalnum())\n",
    "    \n",
    "    model = pd.read_csv(modelDir + \"model.csv\", index_col=[0,1])\n",
    "    model[f_c_] = np.nan\n",
    "    \n",
    "    \n",
    "    \n",
    "    f = pd.read_csv(file_, index_col=0)\n",
    "    if 'CIQID' in f.columns:\n",
    "        del f['CIQID']\n",
    "        \n",
    "    f = f.stack().to_frame()\n",
    "    f.columns = [f_c_]\n",
    "    \n",
    "    model.update(f)\n",
    "    model.sort_index()\n",
    "\n",
    "    model.to_csv(outputDir + f_c_ + \".csv\",index=False) \n",
    "    \n",
    "    del f\n",
    "    del model\n",
    "    gc.collect()\n",
    "\n",
    "\n",
    "n_cores = multiprocessing.cpu_count()\n",
    "filesList = glob.glob(Dir_X + \"*.csv\")\n",
    "data = filesList\n",
    "    \n",
    "func_partX = partial(mp_worker, modelDir=Dir_Model, workingDir=Dir_X, outputDir=Output_Dir_X)\n",
    "func_partY = partial(mp_worker, modelDir=Dir_Model, workingDir=Dir_Y, outputDir=Output_Dir_Y)\n",
    "\n",
    "p = multiprocessing.Pool(Multithread_gran)\n",
    "p.map(func_partX, data)\n",
    "\n",
    "\n",
    "filesList = glob.glob(Dir_Y + \"*.csv\")\n",
    "data = filesList\n",
    "p.map(func_partY, data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filesList = glob.glob(Dir_Y + \"*.csv\")\n",
    "data = filesList\n",
    "p.map(func_partY, data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare csv of Y-type variables (multithread)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import multiprocessing\n",
    "import time\n",
    "import glob\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import gc\n",
    "\n",
    "n_cores = multiprocessing.cpu_count()\n",
    "filesList = glob.glob(Dir_Y + \"*.csv\")\n",
    "data = filesList\n",
    "\n",
    "def mp_worker(file_):\n",
    "    process = multiprocessing.current_process()\n",
    "    print \"File: \" + str(file_) \n",
    "    \n",
    "    \n",
    "    f_c = file_.replace('.csv','')\n",
    "    f_c = f_c.replace(Dir_Y,'')\n",
    "    f_c_ = ''.join(e for e in f_c if e.isalnum())\n",
    "\n",
    "\n",
    "    test = pd.read_csv(file_, index_col=0)\n",
    "    \n",
    "    #@TODO: Rustine to BE DROPPED once fixed in data source\n",
    "    if 'CIQID' in test.columns:\n",
    "        del test['CIQID']\n",
    "    \n",
    "    #test = test.fillna(-1)\n",
    "    test = test.stack(dropna=False).to_frame()\n",
    "\n",
    "    model = pd.read_csv(Dir_Model + \"model.csv\", index_col=[0,1])\n",
    "\n",
    "    df = pd.DataFrame(index=model.index.difference(test.index),data=np.nan,columns=[0])\n",
    "   \n",
    "    frames = [test, df]\n",
    "    bDF = pd.concat(frames)\n",
    "    bDF.sort_index()\n",
    "    bDF.columns = [f_c_]\n",
    "\n",
    "    #bDF.to_csv(\"X_output/standalone/\"+f_c_+\".csv\",index=True)\n",
    "    bDF.to_csv(Output_Dir_Y + f_c_ + \".csv\",index=False) \n",
    "    \n",
    "    del df\n",
    "    del bDF\n",
    "    del model\n",
    "    del test\n",
    "    gc.collect()\n",
    "    \n",
    "p = multiprocessing.Pool(Multithread_gran)\n",
    "p.map(mp_worker, data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## Append all CSV together - merged_X, header_X, merged_Y, header_Y (HDFS ready)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from glob import glob\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "ListF = glob(Output_Dir_X + '*.csv')\n",
    "ListFiltered = []\n",
    "\n",
    "i = 0\n",
    "total = len(ListF)\n",
    "print \"X packages\\n\"\n",
    "\n",
    "with open(Export_name + '_X.csv', 'w') as singleFile:\n",
    "    for csv in ListF :\n",
    "        i = i + 1 \n",
    "        print \"Work: #\" + str(i) + \" / \" + str(total) \n",
    "        if csv == Output_Dir_X + Export_name + '_X.csv':\n",
    "            pass\n",
    "        else:\n",
    "            ListFiltered.append(csv.replace(Output_Dir_X, '').replace('.csv', ''))\n",
    "            for line in open(csv, 'r'):\n",
    "                singleFile.write(line)\n",
    "                \n",
    "arr = np.array(ListFiltered)\n",
    "df = pd.DataFrame(data=arr)\n",
    "df.to_csv(Export_name + \"_X_header.csv\",index=False, header=False)\n",
    "\n",
    "print \"Y packages\\n\"\n",
    "ListF = glob(Output_Dir_Y + '*.csv')\n",
    "ListFiltered = []\n",
    "\n",
    "i = 0\n",
    "total = len(ListF)\n",
    "with open(Export_name + '_Y.csv', 'w') as singleFile:\n",
    "    for csv in ListF :\n",
    "        i = i + 1 \n",
    "        print \"Work: #\" + str(i) + \" / \" + str(total) \n",
    "        if csv == Output_Dir_Y + Export_name + '_Y.csv':\n",
    "            pass\n",
    "        else:\n",
    "            ListFiltered.append(csv.replace(Output_Dir_Y, '').replace('.csv', ''))\n",
    "            for line in open(csv, 'r'):\n",
    "                singleFile.write(line)\n",
    "                \n",
    "arr = np.array(ListFiltered)\n",
    "df = pd.DataFrame(data=arr)\n",
    "df.to_csv(Export_name + \"_Y_header.csv\",index=False, header=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GENERATE SHELL CODE TO EXPORT TO HDFS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Source_Dir_X = Output_Dir_X\n",
    "Source_Dir_Y = Output_Dir_Y\n",
    "\n",
    "\n",
    "#import os\n",
    "print \"############################################################################\"\n",
    "print \"##### Commandes à input dans un shell d'une machine connectée à Hadoop #####\"\n",
    "print \"############################################################################\"\n",
    "print \"\" \n",
    "\n",
    "print 'if $(! hdfs dfs -test -d ' + HDFS_Dir + ') ; then hdfs dfs -mkdir -p ' + HDFS_Dir + '; fi'\n",
    "print 'if $(! hdfs dfs -test -d ' + HDFS_Dir + ') ; then hdfs dfs -mkdir -p ' + HDFS_Dir + '; fi'\n",
    "\n",
    "print \"\"\n",
    "print \"hdfs dfs -put -f \" + Export_name + \"_X.csv \" + HDFS_Dir\n",
    "print \"hdfs dfs -put -f \" + Export_name + \"_X_header.csv \" + HDFS_Dir\n",
    "print \"hdfs dfs -put -f \" + Export_name + \"_Y.csv \" + HDFS_Dir\n",
    "print \"hdfs dfs -put -f \" + Export_name + \"_Y_header.csv \" + HDFS_Dir\n",
    "print \"print 'done'\"\n",
    "#os.system(\"ls\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GENERATE SHELL CODE TO EXPORT TO HDFS (Experimental only)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Source_Dir_X = Output_Dir_X\n",
    "Source_Dir_Y = Output_Dir_Y\n",
    "\n",
    "\n",
    "#import os\n",
    "print \"############################################################################\"\n",
    "print \"##### Commandes à input dans un shell d'une machine connectée à Hadoop #####\"\n",
    "print \"############################################################################\"\n",
    "print \"\" \n",
    "\n",
    "print \"#############################################################################################\"\n",
    "print \"##### CES COMMANDES UPLOAD TOUS LES PETITS FICHIERS ET NON PAS LE GROS FICHIER FUSIONNE #####\"\n",
    "print \"#############################################################################################\"\n",
    "print \"\" \n",
    "\n",
    "print 'if $(! hdfs dfs -test -d ' + HDFS_Dir_X + ') ; then hdfs dfs -mkdir ' + HDFS_Dir_X + '; fi'\n",
    "print 'if $(! hdfs dfs -test -d ' + HDFS_Dir_Y + ') ; then hdfs dfs -mkdir ' + HDFS_Dir_Y + '; fi'\n",
    "\n",
    "print \"\"\n",
    "print \"hdfs dfs -put -f \" + Source_Dir_X +\"*.csv \" + HDFS_Dir_X \n",
    "print \"hdfs dfs -put -f \" + Source_Dir_Y +\"*.csv \" + HDFS_Dir_Y \n",
    "\n",
    "#os.system(\"ls\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
