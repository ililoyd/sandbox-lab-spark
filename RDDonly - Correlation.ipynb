{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.mllib.linalg import Vector, Vectors\n",
    "import numpy as np\n",
    "from pyspark.mllib.stat import Statistics\n",
    "import pandas as pd\n",
    "from hdfs import Config\n",
    "from pyspark.mllib.stat import Statistics\n",
    "import time\n",
    "from itertools import islice\n",
    "\n",
    "\n",
    "######## CONFIG #######################\n",
    "CLUSTER_ADD = \"hdfs://ad-cluster-master:54310\"\n",
    "SOURCE_DIR = \"/user/cluster/source_wfile/\"\n",
    "MODEL_DIR = \"/user/cluster/\"\n",
    "PARQUET_SOURCE_FILE = \"DFSaverBi.parquet\"\n",
    "PARQUET_SOURCE_DIR = \"/user/cluster/\"\n",
    "\n",
    "num_cores = 10\n",
    "gran = 3\n",
    "Force_regen = False\n",
    "\n",
    "#########################################################################\n",
    "client = Config().get_client()\n",
    "\n",
    "\n",
    "Parquet_List = client.list(PARQUET_SOURCE_DIR)\n",
    "\n",
    "\n",
    "regen = False if (PARQUET_SOURCE_FILE in Parquet_List) else True\n",
    "\n",
    "\n",
    "if regen == True or Force_regen == True :\n",
    "    t1 = time.time()\n",
    "    client = Config().get_client()\n",
    "    FilesList = client.list(SOURCE_DIR)\n",
    "    #FilesList = FilesList[:3] #Debug\n",
    "    total = len(FilesList)\n",
    "\n",
    "    ########## DOUBLE INDEX ####################\n",
    "    rdd = sc.textFile(CLUSTER_ADD + MODEL_DIR + 'model.csv',gran*num_cores)\n",
    "    rddDate = rdd.zipWithIndex().filter(lambda ind: ind[1] > 0).map(lambda (x,y): (y,x)).map(lambda x: (x[0],x[1].split(\",\")[0]))\n",
    "    rddAction = rdd.zipWithIndex().filter(lambda ind: ind[1] > 0).map(lambda (x,y): (y,x)).map(lambda x: (x[0],x[1].split(\",\")[1]))\n",
    "    \n",
    "    I = [rddDate, rddAction]\n",
    "    i=0\n",
    "    header = [\"Date\", \"Action\"]\n",
    "    for file_ in FilesList:\n",
    "        i=i+1\n",
    "        print(\"#########\")\n",
    "        print(\"Work: \" + str(i) + \"/\" + str(total))\n",
    "\n",
    "        rddFile = sc.textFile(CLUSTER_ADD + SOURCE_DIR + file_,gran*num_cores)\n",
    "        header.append(rddFile.take(1)[0])\n",
    "\n",
    "        rddT = rddFile.zipWithIndex().filter(lambda ind: ind[1] > 0).map(lambda (x,y): (y,float(x)))\n",
    "\n",
    "        I.append(rddT)\n",
    "\n",
    "    \n",
    "    t2 = time.time()\n",
    "    print(\"####################################\")\n",
    "    print(\"TIME TAKEN FOR TASK:\" + str(t2-t1))\n",
    "\n",
    "    print(\"#############################\")\n",
    "    print(\"FILE LOADING TASK FINISHED\")\n",
    "    print(\"\\n\")\n",
    "    \n",
    "    #######################\n",
    "    \n",
    "    print(\"#############################\")\n",
    "    print(\"TASK: RDD to DF:\")\n",
    "    t1 = time.time()\n",
    "    uni_ = sc.union(I)\n",
    "\n",
    "    s = uni_.groupByKey().mapValues(tuple).map(lambda x: x[1])\n",
    "\n",
    "    df = s.toDF(header)\n",
    "\n",
    "    t2 = time.time()\n",
    "    print(\"####################################\")\n",
    "    print(\"TIME TAKEN FOR TASK:\" + str(t2-t1))\n",
    "\n",
    "    print(\"#############################\")\n",
    "    print(\"RDD to DF TASK FINISHED\")\n",
    "    print(\"\\n\")\n",
    "    \n",
    "    ###################\"\n",
    "    \n",
    "    print(\"#############################\")\n",
    "    print(\"TASK: Save Aggregated to HDFS:\")\n",
    "    t1 = time.time()\n",
    "    df.write.parquet( CLUSTER_ADD + PARQUET_SOURCE_DIR + PARQUET_SOURCE_FILE)\n",
    "    t2 = time.time()\n",
    "    print(\"####################################\")\n",
    "    print(\"TIME TAKEN FOR TASK:\" + str(t2-t1))\n",
    "\n",
    "    print(\"#############################\")\n",
    "    print(\"Save Aggregated to HDFS TASK FINISHED\")\n",
    "    print(\"\\n\")\n",
    "    \n",
    "    check_integrity(df,FilesList)\n",
    "    \n",
    "else:\n",
    "    print(\"LOADING PARQUET FILE DATAFRAME\")\n",
    "    t1 = time.time()\n",
    "    df = spark.read.parquet( CLUSTER_ADD + PARQUET_SOURCE_DIR + PARQUET_SOURCE_FILE)\n",
    "    t2 = time.time()\n",
    "    print(\"####################################\")\n",
    "    print(\"TIME TAKEN FOR TASK:\" + str(t2-t1))\n",
    "\n",
    "    print(\"#############################\")\n",
    "    print(\"Parquet loading TASK FINISHED\")\n",
    "    print(\"\\n\")\n",
    "    \n",
    "print(\"Dataframe LOADED\")\n",
    "\n",
    "dfPersist = df.persist(StorageLevel.MEMORY_AND_DISK)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfPersist.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfPersist.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t1 = time.time()\n",
    "dfPersist.filter(dfPersist.Action == 'IQ103375').filter(dfPersist.Date < '2012-08-20')\\\n",
    ".filter(dfPersist.Date > '2010-08-20').orderBy(\"Date\").select(\"Date\", \"Action\").show()\n",
    "t2 = time.time()\n",
    "\n",
    "print t2 - t1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd = sc.textFile(CLUSTER_ADD + MODEL_DIR + 'model.csv',4*num_cores)\n",
    "print(rdd.count())\n",
    "print(rdd.take(3))\n",
    "rddDate = rdd.zipWithIndex().filter(lambda ind: ind[1] > 0).map(lambda (x,y): (y,x)).map(lambda x: (x[0],x[1].split(\",\")[0]))\n",
    "print rddDate.take(4)\n",
    "\n",
    "rddAction = rdd.zipWithIndex().filter(lambda ind: ind[1] > 0).map(lambda (x,y): (y,x)).map(lambda x: (x[0],x[1].split(\",\")[1]))\n",
    "print rddAction.take(4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = Config().get_client()\n",
    "FilesList = client.list(SOURCE_DIR)\n",
    "\n",
    "a = FilesList[0]\n",
    "rdd2 = sc.textFile(CLUSTER_ADD + SOURCE_DIR + a,4*num_cores)\n",
    "print(rdd2.count())\n",
    "print(rdd2.take(3))\n",
    "rdd2 = rdd2.zipWithIndex().filter(lambda ind: ind[1] > 0).map(lambda (x,y): (y,float(x)))\n",
    "rdd2.take(4)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "u = [rddDate, rddAction, rdd2]\n",
    "u"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "te = sc.union(u)\n",
    "\n",
    "#tup0 = index created\n",
    "#tup1 = data\n",
    "#map data to float \n",
    "st = te.groupByKey().mapValues(tuple).map(lambda x: x[1])\n",
    "st.take(4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rddDate.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd2.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dft = st.toDF([\"date\", \"action\" ,\"X1\"])\n",
    "dft.orderBy(\"date\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "h1 = Mat3.columns[125]\n",
    "h2 = Mat3.columns[302]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Mat3.stat.corr(h1,h2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfPersist.rdd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Mat3.write.parquet(CLUSTER_ADD+\"/user/cluster/dataFSave.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = s.map(lambda x: (1, len(x))).countByValue()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "header = []\n",
    "I = []\n",
    "\n",
    "for file_ in FileList:\n",
    "    # CHARGEMENT DU FICHIER CSV\n",
    "    rddFile = sc.textFile(CLUSTER_ADD + SOURCE_DIR + file_,gran*num_cores)\n",
    "    \n",
    "    # On récupère le nom de la variable (1e ligne)\n",
    "    header.append(rddFile.take(1)[0])\n",
    "\n",
    "    rddT = rddFile.zipWithIndex()\\ ### AJOUTE UN INDEX FICTIF de 1 à 3M\n",
    "                    .filter(lambda ind: ind[1] > 0)\\ ### On retire la 1e ligne (nom de la variable)\n",
    "                    .map(lambda (x,y): (y,float(x))) ### On inverse les 2 colonnes et on force la conversion en float\n",
    "\n",
    "    I.append(rddT) # On met dans la liste\n",
    "    \n",
    "uni_ = sc.union(I) ### Concatène les RDD, list[RDD] -> UnionRDD[float]\n",
    "\n",
    "s = uni_.groupByKey()\\ ### On regroupe par clé, ici l'index on obtient RDD (key => Iterator[X1,X2...,X465,X466])\n",
    "    .mapValues(tuple)\\ ### On transforme l'itérateur en tuple, RDD = (key,(X1,...,X466))\n",
    "    .map(lambda x: x[1]) ### On conserve uniquement le tuple\n",
    "    \n",
    "df = s.toDF(header) ### Conversion en Dataframe\n",
    "\n",
    "#### 2 Transformations: GroupByKey (shuffle transfo) et toDF (normal transfo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = dfPersist.select(h[2:])\n",
    "df.select(h[2:]).rdd.map(lambda data: Vectors.dense([float(c) for c in data]))\n",
    "\n",
    "df.take(2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t1 = time.time()\n",
    "h = dfPersist.columns\n",
    "rddTest = dfPersist.select(h[2:]).rdd.map(lambda data: Vectors.dense([float(c) for c in data]))\n",
    "A = Statistics.corr(rddTest)\n",
    "A\n",
    "        \n",
    "t2 = time.time()\n",
    "print(\"####################################\")\n",
    "print(\"TIME TAKEN FOR TASK:\" + str(t2-t1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_ = FilesList[0]\n",
    "\n",
    "\n",
    "file_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "df1 = pd.read_csv(CLUSTER_ADD + SOURCE_DIR + file_)\n",
    "df1.head()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "client = Config().get_client()\n",
    "FilesList = client.list(SOURCE_DIR)\n",
    "header = dfPersist.columns\n",
    "seed = time.time()\n",
    "random.seed(seed)\n",
    "num_check = int(len(FilesList) * 1 / 10)\n",
    "print(num_check)\n",
    "\n",
    "for i in range(len(FilesList)):\n",
    "    id = random.randint(0,len(FilesList) - 1)\n",
    "    print(\"WORK: \" + str(i) + \" ; \" + str(id))\n",
    "    \n",
    "    with client.read(SOURCE_DIR + FilesList[i]) as reader:\n",
    "        dfPandas = pd.read_csv(reader)\n",
    "    \n",
    "    dfCheck = dfPersist.orderBy(\"Date\", \"Action\").select([header[2+i]]).toPandas()\n",
    "    assert dfPandas.equals(dfCheck), \"Integrity Check Failure: \" + str(header[2+id]) + \"seed: \" + str(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfCheck = dfPersist.orderBy(\"Date\", \"Action\").select([header[2+id]]).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dfPandas.sort_values(header[2+id],ascending=False)\n",
    "dfPandas.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfCheck.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert dfPandas.equals(dfCheck), \"Integrity Check Failure: \" + str(header[2+id]) + \"seed: \" + str(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "client = Config().get_client()\n",
    "FilesList = client.list(SOURCE_DIR)\n",
    "header = dfPersist.columns\n",
    "t1 = time.time()\n",
    "columns = header[2:]\n",
    "t = len(columns)\n",
    "\n",
    "MatCheck = A\n",
    "\n",
    "for i in range(t):\n",
    "    print(\"Line \" + str(i))\n",
    "    with client.read(SOURCE_DIR + FilesList[i]) as reader:\n",
    "        dfP1 = pd.read_csv(reader)\n",
    "    for j in range(i+1,t):\n",
    "        if (j % 100 == 0):\n",
    "            print(\"     Col \" + str(j))\n",
    "        with client.read(SOURCE_DIR + FilesList[j]) as reader:\n",
    "            dfP2 = pd.read_csv(reader)\n",
    "            corr = dfP2.corrwith(dfP1[header[2+i]])\n",
    "            assert ( (MatCheck[i,j] - corr[0]) < 10e-5), \"Correlation Failure: (i,j): (\" + str(i) + \";\" + str(j) +\"); delta = \" + str(MatCheck[i,j] - corr[0])         \n",
    "t2 = time.time()\n",
    "print(\"####################################\")\n",
    "print(\"TIME TAKEN FOR TASK:\" + str(t2-t1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del dfPandas\n",
    "del dfCheck"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfPersist.select(header[1]).withColumn(\"test\",str(\"Action\") + str(col)).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "col = dfPersist.Action\n",
    "col"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
