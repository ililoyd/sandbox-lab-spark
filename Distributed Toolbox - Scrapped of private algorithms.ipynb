{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Import Into HDFS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "Source_Dir_X = \"src/X_output/workfile/\"\n",
    "HDFS_Dir_X = \"/user/cluster/AdLearn/Dev/workfile2/\"\n",
    "Source_Dir_Y = \"src/Y_output/\"\n",
    "HDFS_Dir_Y =\"/user/cluster/AdLearn/Dev/Y2/\"\n",
    "\n",
    "#import os\n",
    "print \"############################################################################\"\n",
    "print \"##### Commandes à input dans un shell d'une machine connectée à Hadoop #####\"\n",
    "print \"############################################################################\"\n",
    "print \"\" \n",
    "\n",
    "print 'if $(! hdfs dfs -test -d ' + HDFS_Dir_X + ') ; then hdfs dfs -mkdir ' + HDFS_Dir_X + '; fi'\n",
    "print 'if $(! hdfs dfs -test -d ' + HDFS_Dir_Y + ') ; then hdfs dfs -mkdir ' + HDFS_Dir_Y + '; fi'\n",
    "\n",
    "print \"\"\n",
    "print \"hdfs dfs -put -f \" + Source_Dir_X +\"*.csv \" + HDFS_Dir_X \n",
    "print \"hdfs dfs -put -f \" + Source_Dir_Y +\"*.csv \" + HDFS_Dir_Y \n",
    "\n",
    "#os.system(\"ls\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "sc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Configuration Zone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "CLUSTER_ADD = \"hdfs://ad-cluster-master:54310\"\n",
    "SOURCE_DIR = \"/user/cluster/AdLearn/Debug/17-05-18/\"\n",
    "MODEL_DIR = \"/user/cluster/AdLearn/Debug/\"\n",
    "Y_DIR = \"/user/cluster/AdLearn/Debug/17-05-18/\"\n",
    "PARQUET_SOURCE_FILE = \"AdDF-debug12.parquet\"\n",
    "PARQUET_SOURCE_DIR = \"/user/cluster/AdLearn/Debug/\"\n",
    "Y_FILE = \"EW_YF3MCN.csv\"\n",
    "\n",
    "num_cores = 18\n",
    "gran = 2\n",
    "Force_regen = False\n",
    "\n",
    "BaseNameMergedFile = \"merged_forHDFS2\"\n",
    "Const_HEADER = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "from pyspark.mllib.linalg import Vector, Vectors\n",
    "from pyspark.sql.functions import col\n",
    "import numpy as np\n",
    "from pyspark.mllib.stat import Statistics\n",
    "import pandas as pd\n",
    "from hdfs import Config\n",
    "from pyspark.mllib.stat import Statistics\n",
    "import time\n",
    "from itertools import islice\n",
    "from pyspark.mllib.linalg.distributed import *\n",
    "import sys"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# (To Be Reduced) Aggregation into HDFS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from pyspark.mllib.linalg import Vector, Vectors\n",
    "from pyspark.sql.functions import col\n",
    "import numpy as np\n",
    "from pyspark.mllib.stat import Statistics\n",
    "import pandas as pd\n",
    "from hdfs import Config\n",
    "from pyspark.mllib.stat import Statistics\n",
    "import time\n",
    "from itertools import islice\n",
    "from pyspark.mllib.linalg.distributed import *\n",
    "import sys\n",
    "\n",
    "try:\n",
    "    Const_HEADER\n",
    "except NameError:\n",
    "    sys.exit(\"Loading Configuration is needed !\")\n",
    "    \n",
    "client = Config().get_client()\n",
    "Parquet_List = client.list(PARQUET_SOURCE_DIR)\n",
    "regen = False if (PARQUET_SOURCE_FILE in Parquet_List) else True\n",
    "\n",
    "\n",
    "if regen == True or Force_regen == True :\n",
    "    t1 = time.time()\n",
    "    print \"##########################\"\n",
    "    print \"#### TASK: Files gathering\"\n",
    "    print \"Model Loading\"\n",
    "    rdd = sc.textFile(CLUSTER_ADD + MODEL_DIR + 'model.csv')\n",
    "    rddDate = rdd.zipWithIndex().filter(lambda ind: ind[1] > 0).map(lambda (x,y): (y,x)).map(lambda x: (x[0],x[1].split(\",\")[0]))\n",
    "    rddAction = rdd.zipWithIndex().filter(lambda ind: ind[1] > 0).map(lambda (x,y): (y,x)).map(lambda x: (x[0],x[1].split(\",\")[1]))\n",
    "\n",
    "    I = [rddDate, rddAction]\n",
    "    header = [\"Date\", \"Ids\"]\n",
    "\n",
    "    n = rddDate.count() + 1\n",
    "\n",
    "\n",
    "    ######## RDDX ################\n",
    "    print \"X vars Loading\"\n",
    "    rddFile = sc.textFile(CLUSTER_ADD + SOURCE_DIR + BaseNameMergedFile + '_X.csv', (gran+1) * num_cores)\n",
    "    rddX_ = rddFile.map(lambda x: np.nan if x=='\"\"' else x).zipWithIndex().map(lambda x: (x[1] % n , x[0]))\n",
    "    rddX  = rddX_.filter(lambda ind: ind[0] > 0).map(lambda (x,y): (x,float(y)))\n",
    "\n",
    "    header_ = sc.textFile(CLUSTER_ADD + SOURCE_DIR + BaseNameMergedFile + '_X_header.csv').collect()\n",
    "\n",
    "    header = header + header_\n",
    "\n",
    "    I.append(rddX)\n",
    "\n",
    "    ######## RDDY ###############\n",
    "    print \"Y vars Loading\"\n",
    "    rddFile = sc.textFile(CLUSTER_ADD + Y_DIR + BaseNameMergedFile +'_Y.csv')\n",
    "\n",
    "    rddY_ = rddFile.map(lambda x: np.nan if x=='\"\"' else x).zipWithIndex().map(lambda x: (x[1] % n , x[0]))\n",
    "    rddY  = rddY_.filter(lambda ind: ind[0] > 0).map(lambda (x,y): (x,float(y)))\n",
    "\n",
    "    header_ = sc.textFile(CLUSTER_ADD + Y_DIR  + BaseNameMergedFile + '_Y_header.csv').collect()\n",
    "\n",
    "    header = header + header_\n",
    "\n",
    "    I.append(rddY)\n",
    "    \n",
    "    t2 = time.time()\n",
    "    print(\"####################################\")\n",
    "    print(\"TIME TAKEN FOR TASK:\" + str(t2-t1))\n",
    "\n",
    "    print(\"#############################\")\n",
    "    print(\"FILE LOADING TASK FINISHED\")\n",
    "    print(\"\\n\")\n",
    "    \n",
    "    #######################\n",
    "    \n",
    "    print(\"#############################\")\n",
    "    print(\"TASK: RDD to DF:\")\n",
    "    t1 = time.time()\n",
    "    \n",
    "    uni2 =  sc.union(I).zipWithIndex().map(lambda ind: [ind[0][0],(ind[1], ind[0][1])]).coalesce((gran) * num_cores)\n",
    "\n",
    "    def multipleJoins(x,y):\n",
    "        if isinstance(x, list) and isinstance(y, list):\n",
    "            return sorted(x + y)\n",
    "        elif isinstance(x, list):\n",
    "            return sorted(x + [y])\n",
    "        elif isinstance(y, list):\n",
    "            return sorted([x] + y)\n",
    "        else:\n",
    "            return sorted([x,y])\n",
    "\n",
    "    s = uni2.reduceByKey(multipleJoins).map(lambda x: x[1]).map(lambda tuples: [x[1] for x in tuples])\n",
    "    #s = uni2.groupByKey().mapValues(list).map(lambda data: sorted(data[1], key=lambda x: x[0])).map(lambda tuples: [x[1] for x in tuples])\n",
    "\n",
    "    df = s.toDF(header)\n",
    "\n",
    "    t2 = time.time()\n",
    "    print(\"####################################\")\n",
    "    print(\"TIME TAKEN FOR TASK:\" + str(t2-t1))\n",
    "\n",
    "    print(\"#############################\")\n",
    "    print(\"RDD to DF TASK FINISHED\")\n",
    "    print(\"\\n\")\n",
    "    \n",
    "    ###################\"\n",
    "    \n",
    "    print(\"#############################\")\n",
    "    print(\"TASK: Save Aggregated to HDFS:\")\n",
    "    t1 = time.time()\n",
    "    df.write.parquet( CLUSTER_ADD + PARQUET_SOURCE_DIR + PARQUET_SOURCE_FILE)\n",
    "    t2 = time.time()\n",
    "    print(\"####################################\")\n",
    "    print(\"TIME TAKEN FOR TASK:\" + str(t2-t1))\n",
    "\n",
    "    print(\"#############################\")\n",
    "    print(\"Save Aggregated to HDFS TASK FINISHED\")\n",
    "    print(\"\\n\")\n",
    "\n",
    "print \"FILE AGGREGATION FINISHED\"\n",
    "#check_integrity(df,FilesList)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "from pyspark.mllib.linalg import Vector, Vectors\n",
    "from pyspark.sql.functions import col\n",
    "import numpy as np\n",
    "from pyspark.mllib.stat import Statistics\n",
    "import pandas as pd\n",
    "from hdfs import Config\n",
    "from pyspark.mllib.stat import Statistics\n",
    "import time\n",
    "from itertools import islice\n",
    "from pyspark.mllib.linalg.distributed import *\n",
    "import sys\n",
    "\n",
    "try:\n",
    "    Const_HEADER\n",
    "except NameError:\n",
    "    sys.exit(\"Loading Configuration is needed !\")\n",
    "    \n",
    "client = Config().get_client()\n",
    "Parquet_List = client.list(PARQUET_SOURCE_DIR)\n",
    "regen = False if (PARQUET_SOURCE_FILE in Parquet_List) else True\n",
    "\n",
    "\n",
    "t1 = time.time()\n",
    "print \"##########################\"\n",
    "print \"#### TASK: Files gathering\"\n",
    "print \"Model Loading\"\n",
    "rdd = sc.textFile(CLUSTER_ADD + MODEL_DIR + 'model.csv')\n",
    "rddDate = rdd.zipWithIndex().filter(lambda ind: ind[1] > 0).map(lambda (x,y): (y,x)).map(lambda x: (x[0],x[1].split(\",\")[0]))\n",
    "rddAction = rdd.zipWithIndex().filter(lambda ind: ind[1] > 0).map(lambda (x,y): (y,x)).map(lambda x: (x[0],x[1].split(\",\")[1]))\n",
    "\n",
    "I = [rddDate, rddAction]\n",
    "header = [\"Date\", \"Ids\"]\n",
    "\n",
    "n = rddDate.count() + 1\n",
    "\n",
    "\n",
    "######## RDDX ################\n",
    "print \"X vars Loading\"\n",
    "rddFile = sc.textFile(CLUSTER_ADD + SOURCE_DIR + BaseNameMergedFile + '_X.csv', (gran+1) * num_cores)\n",
    "rddX_ = rddFile.map(lambda x: np.nan if x=='\"\"' else x).zipWithIndex().map(lambda x: (x[1] % n , x[0]))\n",
    "rddX  = rddX_.filter(lambda ind: ind[0] > 0).map(lambda (x,y): (x,float(y)))\n",
    "\n",
    "header_ = sc.textFile(CLUSTER_ADD + SOURCE_DIR + BaseNameMergedFile + '_X_header.csv').collect()\n",
    "\n",
    "header = header + header_\n",
    "\n",
    "I.append(rddX)\n",
    "\n",
    "######## RDDY ###############\n",
    "print \"Y vars Loading\"\n",
    "rddFile = sc.textFile(CLUSTER_ADD + Y_DIR + BaseNameMergedFile +'_Y.csv')\n",
    "\n",
    "rddY_ = rddFile.map(lambda x: np.nan if x=='\"\"' else x).zipWithIndex().map(lambda x: (x[1] % n , x[0]))\n",
    "rddY  = rddY_.filter(lambda ind: ind[0] > 0).map(lambda (x,y): (x,float(y)))\n",
    "\n",
    "header_ = sc.textFile(CLUSTER_ADD + Y_DIR  + BaseNameMergedFile + '_Y_header.csv').collect()\n",
    "\n",
    "header = header + header_\n",
    "\n",
    "I.append(rddY)\n",
    "\n",
    "t2 = time.time()\n",
    "print(\"####################################\")\n",
    "print(\"TIME TAKEN FOR TASK:\" + str(t2-t1))\n",
    "\n",
    "print(\"#############################\")\n",
    "print(\"FILE LOADING TASK FINISHED\")\n",
    "print(\"\\n\")\n",
    "\n",
    "#######################\n",
    "\n",
    "print(\"#############################\")\n",
    "print(\"TASK: RDD to DF:\")\n",
    "t1 = time.time()\n",
    "\n",
    "uni2 =  sc.union(I).zipWithIndex().map(lambda ind: [ind[0][0],(ind[1], ind[0][1])]).coalesce((gran) * num_cores)\n",
    "\n",
    "def multipleJoins(x,y):\n",
    "    if isinstance(x, list) and isinstance(y, list):\n",
    "        return sorted(x + y)\n",
    "    elif isinstance(x, list):\n",
    "        return sorted(x + [y])\n",
    "    elif isinstance(y, list):\n",
    "        return sorted([x] + y)\n",
    "    else:\n",
    "        return sorted([x,y])\n",
    "\n",
    "s = uni2.reduceByKey(multipleJoins)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# (Deprecated) Aggregation into HDFS - For documentation purpose"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "from pyspark.mllib.linalg import Vector, Vectors\n",
    "from pyspark.sql.functions import col\n",
    "import numpy as np\n",
    "from pyspark.mllib.stat import Statistics\n",
    "import pandas as pd\n",
    "from hdfs import Config\n",
    "from pyspark.mllib.stat import Statistics\n",
    "import time\n",
    "from itertools import islice\n",
    "from pyspark.mllib.linalg.distributed import *\n",
    "\n",
    "\n",
    "#########################################################################\n",
    "client = Config().get_client()\n",
    "\n",
    "\n",
    "Parquet_List = client.list(PARQUET_SOURCE_DIR)\n",
    "\n",
    "\n",
    "regen = False if (PARQUET_SOURCE_FILE in Parquet_List) else True\n",
    "\n",
    "\n",
    "if regen == True or Force_regen == True :\n",
    "    t1 = time.time()\n",
    "    client = Config().get_client()\n",
    "    FilesList = client.list(SOURCE_DIR)\n",
    "    #FilesList = FilesList[:3] #Debug\n",
    "    total = len(FilesList)\n",
    "\n",
    "    ########## DOUBLE INDEX ####################\n",
    "    rdd = sc.textFile(CLUSTER_ADD + MODEL_DIR + 'model.csv')\n",
    "    rddDate = rdd.zipWithIndex().filter(lambda ind: ind[1] > 0).map(lambda (x,y): (y,x)).map(lambda x: (x[0],x[1].split(\",\")[0]))\n",
    "    rddAction = rdd.zipWithIndex().filter(lambda ind: ind[1] > 0).map(lambda (x,y): (y,x)).map(lambda x: (x[0],x[1].split(\",\")[1]))\n",
    "    \n",
    "    I = [rddDate, rddAction]\n",
    "    i=0\n",
    "    header = [\"Date\", \"Ids\"]\n",
    "    for file_ in FilesList:\n",
    "        i=i+1\n",
    "        print(\"#########\")\n",
    "        print(\"Work: \" + str(i) + \"/\" + str(total))\n",
    "\n",
    "        rddFile = sc.textFile(CLUSTER_ADD + SOURCE_DIR + file_)\n",
    "        header.append(rddFile.take(1)[0])\n",
    "\n",
    "        rddT = rddFile.map(lambda x: np.nan if x=='\"\"' else x).zipWithIndex().filter(lambda ind: ind[1] > 0).map(lambda (x,y): (y,float(x)))\n",
    "\n",
    "        I.append(rddT)\n",
    "        \n",
    "    FilesList2 = client.list(Y_DIR)\n",
    "    \n",
    "    for file_ in FilesList2:\n",
    "        i=i+1\n",
    "        print(\"#########\")\n",
    "        print(\"Work: \" + str(i) + \"/\" + str(total))\n",
    "        rddY = sc.textFile(CLUSTER_ADD + Y_DIR + file_)\n",
    "        header.append(rddY.take(1)[0])\n",
    "        rddY = rddY.map(lambda x: np.nan if x=='\"\"' else x).zipWithIndex().filter(lambda ind: ind[1] > 0).map(lambda (x,y): (y,float(x)))\n",
    "        I.append(rddY)\n",
    "\n",
    "    t2 = time.time()\n",
    "    print(\"####################################\")\n",
    "    print(\"TIME TAKEN FOR TASK:\" + str(t2-t1))\n",
    "\n",
    "    print(\"#############################\")\n",
    "    print(\"FILE LOADING TASK FINISHED\")\n",
    "    print(\"\\n\")\n",
    "    \n",
    "    #######################\n",
    "    \n",
    "    print(\"#############################\")\n",
    "    print(\"TASK: RDD to DF:\")\n",
    "    t1 = time.time()\n",
    "    \n",
    "    uni2 =  sc.union(I).zipWithIndex().map(lambda ind: [ind[0][0],(ind[1], ind[0][1])])\n",
    "\n",
    "    def multipleJoins(x,y):\n",
    "        if isinstance(x, list):\n",
    "            return sorted(x + [y])\n",
    "        else:\n",
    "            return sorted([x,y])\n",
    "\n",
    "    s = uni2.reduceByKey(multipleJoins).map(lambda x: x[1]).map(lambda tuples: [x[1] for x in tuples])\n",
    "\n",
    "    df = s.toDF(header)\n",
    "\n",
    "    t2 = time.time()\n",
    "    print(\"####################################\")\n",
    "    print(\"TIME TAKEN FOR TASK:\" + str(t2-t1))\n",
    "\n",
    "    print(\"#############################\")\n",
    "    print(\"RDD to DF TASK FINISHED\")\n",
    "    print(\"\\n\")\n",
    "    \n",
    "    ###################\"\n",
    "    \n",
    "    print(\"#############################\")\n",
    "    print(\"TASK: Save Aggregated to HDFS:\")\n",
    "    t1 = time.time()\n",
    "    df.write.parquet( CLUSTER_ADD + PARQUET_SOURCE_DIR + PARQUET_SOURCE_FILE)\n",
    "    t2 = time.time()\n",
    "    print(\"####################################\")\n",
    "    print(\"TIME TAKEN FOR TASK:\" + str(t2-t1))\n",
    "\n",
    "    print(\"#############################\")\n",
    "    print(\"Save Aggregated to HDFS TASK FINISHED\")\n",
    "    print(\"\\n\")\n",
    "    \n",
    "    #check_integrity(df,FilesList)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Integrity Test (non retested for new Aggregation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#check_integrity\n",
    "import random\n",
    "client = Config().get_client()\n",
    "FilesList = client.list(SOURCE_DIR)\n",
    "header = df2.columns\n",
    "seed = time.time()\n",
    "random.seed(seed)\n",
    "num_check = int(len(FilesList) * 1 / 10)\n",
    "\n",
    "print(num_check)\n",
    "\n",
    "for i in range(len(FilesList)):\n",
    "    id = random.randint(0,len(FilesList) - 1)\n",
    "    print(\"WORK: \" + str(i) + \" ; \" + str(id))\n",
    "    \n",
    "    with client.read(SOURCE_DIR + FilesList[i]) as reader:\n",
    "        dfPandas = pd.read_csv(reader)\n",
    "    \n",
    "    dfCheck = df2.orderBy(\"Date\", \"Ids\").select([header[2+i]]).toPandas()\n",
    "    assert dfPandas.equals(dfCheck), \"Integrity Check Failure: \" + str(header[2+i]) + \\\n",
    "                                                \"id: \" + str(2+i) +\" seed: \" + str(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Loading Data from HDFS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "print(\"LOADING PARQUET FILE DATAFRAME\")\n",
    "t1 = time.time()\n",
    "df = spark.read.parquet( CLUSTER_ADD + PARQUET_SOURCE_DIR + PARQUET_SOURCE_FILE)\n",
    "t2 = time.time()\n",
    "print(\"####################################\")\n",
    "print(\"TIME TAKEN FOR TASK:\" + str(t2-t1))\n",
    "\n",
    "print(\"#############################\")\n",
    "print(\"Parquet loading TASK FINISHED\")\n",
    "print(\"\\n\")\n",
    "    \n",
    "print(\"Dataframe LOADED\")\n",
    "\n",
    "#dfPersist = df.persist(StorageLevel.MEMORY_AND_DISK)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sampling Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''import re\n",
    "hw_ = df.columns[2:]\n",
    "ChosenVar = hw_.index(\"EWYF3MCN\")\n",
    "\n",
    "h_ = [item for i, item in enumerate(hw_) if not re.search('src*', item)]\n",
    "h_.append(hw_[ChosenVar])\n",
    "h_\n",
    "\n",
    "################\n",
    "\n",
    "header = df.columns\n",
    "dateDebut = df.select(\"Date\",h_[-1]).orderBy(\"Date\").dropna().select(\"Date\").limit(1).rdd.map(lambda x: list(x)[0]).take(1)[0]\n",
    "\n",
    "\n",
    "dfSample = df.filter(col(\"Date\") >= dateDebut).filter(col(\"Date\") < '2016-01-01')\\\n",
    "                .orderBy(\"Date\",\"Ids\").select(header[2:])\n",
    "    \n",
    "dY = dfSample.select(h_[-1])\n",
    "dfSample = dfSample.select(h_).persist()\n",
    "\n",
    "dY.show()\n",
    "dfSample.count()'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ChosenVar = df.columns.index(\"EWYF3MCN\")\n",
    "\n",
    "h_ = df.columns[2:-4]\n",
    "h_.append(df.columns[ChosenVar])\n",
    "h_\n",
    "\n",
    "dateDebut = df.select(\"Date\",h_[-1]).orderBy(\"Date\").dropna().select(\"Date\").limit(1).rdd.map(lambda x: list(x)[0]).take(1)[0]\n",
    "\n",
    "\n",
    "dfSample = df.filter(col(\"Date\") >= dateDebut).filter(col(\"Date\") < '2016-01-01')\\\n",
    "                .orderBy(\"Date\",\"Ids\").select(h_).persist()\n",
    "    \n",
    "dY = dfSample.select(h_[-1])\n",
    "\n",
    "dY.show()\n",
    "dfSample.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfX = dfSample.fillna(0)\n",
    "dYx = dfX.select(dfSample.columns[-1])\n",
    "dYx.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compute ExpY and MeanY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math as m\n",
    "import math\n",
    "\n",
    "t1 = time.time()\n",
    "t = dY.filter(dY[dfSample.columns[-1]] != np.nan).rdd.map(lambda data: Vectors.dense([float(c) for c in data]))\n",
    "b = Statistics.colStats(t)\n",
    "Ecart_Y = m.sqrt(b.variance())\n",
    "Esp_Y = b.mean()[0]\n",
    "t2 = time.time()\n",
    "\n",
    "print(t2-t1)\n",
    "print(Ecart_Y)\n",
    "print(Esp_Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AdLearn Distributed Toolbox"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# take 'rdd' of n * p size with n - 1 features vector and 1 y vector\n",
    "# take 2 extrema for comparison\n",
    "# Takes out y vector\n",
    "# Return activation vector by comparing with the extrema\n",
    "from operator import add; \n",
    "\n",
    "\n",
    "def map_booleanRuling(x, min_b, max_b):\n",
    "    #row = list(x)\n",
    "    row = np.array(x[:-1])\n",
    "    ''' for i in range(len(row)-1):\n",
    "        if (min_b <= row[i]) and (row[i] <= max_b):\n",
    "            row[i] = 1\n",
    "        else:\n",
    "            row[i] = 0\n",
    "    '''\n",
    "    bool_ = np.logical_and(min_b <= row, row <= max_b).astype(int)\n",
    "    \n",
    "    return np.append(bool_, x[-1])\n",
    "\n",
    "# Vectoriel sum\n",
    "# \"Vertical sum\" in terms of rdd\n",
    "# Last step of \"internal\" Dot Product\n",
    "def red_vectSum(x,y):\n",
    "    #out = []\n",
    "    #if type(x) == list and type(y)  == list:       \n",
    "    return np.array(x) + np.array(y)\n",
    "    #return map(add, x, y)\n",
    "    #else:\n",
    "    #    raise Exception(\"RDD wrongly formed !\")\n",
    "        \n",
    "def red_vectSum_withoutLast(x,y):\n",
    "    return red_vectSum(x[:-1],y[:-1])\n",
    "             \n",
    "# Take rdd 'x' and boolean selector of size n - 1.\n",
    "# x is of size n * p is n - 1 features vectors + 1 y vector (last column)\n",
    "# return the m selected vectors of 'x' and y vector in one rdd\n",
    "def map_selectActiv(x,boolCol):\n",
    "    #y = []\n",
    "    \n",
    "    y = np.array(x[:-1])[boolCol]\n",
    "    return np.append(y,x[-1]).tolist()\n",
    "\n",
    "    ''' \n",
    "   for i in range(len(x)-1):\n",
    "        if boolCol[i]:\n",
    "            y.append(x[i])\n",
    "            \n",
    "    y.append(x[len(x)-1])       \n",
    "    return y\n",
    "    '''\n",
    "\n",
    "# take rdd 'x' of size n * p with n - 1 features vectors and 1 y vector\n",
    "# return rdd with each x_i,j element multiplied by associated y_i \n",
    "# First step of \"internal\" Dot Product\n",
    "# Erase y_i column in returned rdd\n",
    "def map_Internal_YProduct(x):\n",
    "    #out = []\n",
    "    #y = x[len(x) - 1]\n",
    "    #for i in range(len(x) - 1):\n",
    "        #out.append(x[i] * y)\n",
    "\n",
    "    return (np.array(x[:-1]) * x[-1])\n",
    "\n",
    "# FACADE function (design pattern)\n",
    "# wrapping up all error function adapted for the current learning\n",
    "def computeError(rdd, esp_cond, nb_act, function = \"MSE\"):\n",
    "    \n",
    "    if function == \"MSE\":\n",
    "        return computeMSE(rdd, esp_cond, nb_act)\n",
    "    else:\n",
    "        raise Exception(\"No valid error function provided !\")\n",
    "\n",
    "        \n",
    "def map_MSE(x, esp):\n",
    "    exp = (esp - x[-1]) ** 2\n",
    "    x = np.array(x[:-1])\n",
    "    return np.multiply(x, exp) \n",
    "        \n",
    "    \n",
    "# MSE loss function\n",
    "def computeMSE(rdd, esp_cond, nb_act):\n",
    "    esp_cond = np.array(esp_cond)\n",
    "    #rdd = rdd.map(lambda x: [x[i] * (esp_cond[i] - x[-1]) ** 2 for i in range(len(x)-1)])\n",
    "    rdd = rdd.map(functools.partial(map_MSE, esp=esp_cond))\n",
    "    \n",
    "\n",
    "    rdd = rdd.reduce(red_vectSum)\n",
    "    array = np.divide(np.array(rdd), nb_act)\n",
    "    \n",
    "    return array\n",
    "\n",
    "def computeCondProb(rdd, nb_act):\n",
    "    #Map = Matrix of (x_i,j * y_i)\n",
    "    #Reduce = Vertical Sum\n",
    "    # Map + Reduce = X . Y_transpose\n",
    "    dotProduct = rdd.map(map_Internal_YProduct).reduce(red_vectSum)\n",
    "    \n",
    "    # X.Y / nb_activation sur les règles qui passent le filtre de coverage\n",
    "    return np.divide(np.array(dotProduct), nb_act)\n",
    "    \n",
    "def computeZscore(prob_cond, nb_act, esp_y, ecart_y):\n",
    "    deltaCond = np.absolute(prob_cond - esp_y)\n",
    "    \n",
    "    # | Esp_Cond_Y - Esp_Y | / (Ecart_Y / Racine(nb_activation)) \n",
    "    Zscore = np.multiply(deltaCond, np.sqrt(nb_act) / ecart_y)\n",
    "    return Zscore\n",
    "        \n",
    "def red_vectSum_withoutY(x,y):\n",
    "    return red_vectSum([x[0]], [y[0]])\n",
    "'''\n",
    "    if type(x) == list and type(y)  == list:\n",
    "        l = max(len(x), len(y))\n",
    "        return red_vectSum([x[0]], [y[0]])\n",
    "    else:\n",
    "        raise Exception(\"RDD wrongly formed !\")\n",
    "'''\n",
    "# take 'rdd' of n * p size with n - 1 features vector and 1 y vector\n",
    "# take 2 extrema for comparison\n",
    "# Takes out y vector\n",
    "# Return activation vector by comparing with the extrema\n",
    "def map_booleanRuling_list(x, min_b, max_b):\n",
    "    row = np.array(x[:-1])\n",
    "    bool_test = np.logical_and(row > np.array(min_b), row < np.array(max_b))\n",
    "    \n",
    "    return np.append(np.where(bool_test,1,0), x[-1])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ter = np.array([6,7,5,1,9,6,5,4,3])\n",
    "mina = [5,8,1,2,5,8,9,4,1]\n",
    "maxa = [9,9,6,5,9,7,6,2,7]\n",
    "bo = np.logical_and(ter > mina, ter < maxa)\n",
    "np.append(np.where(bo,1,0),25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfSample.dropna().select(dfSample.columns[1]).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def constructDictionnaryX(df):\n",
    "    h = df.columns\n",
    "    dictX = {}\n",
    "    total = len(h)\n",
    "    dictX = {x: df.select(x).dropna().count() for x in h}\n",
    "\n",
    "        \n",
    "    return dictX\n",
    "    \n",
    "dictX = constructDictionnaryX(dfSample)\n",
    "dictX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A = dfSample.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, count, sum\n",
    "B = dfSample.na.replace(np.nan, 'NA').agg(*[\n",
    "    count(c).alias(c)    # vertical operations in SQL ignore NULL\n",
    "    for c in dfSample.columns\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ZZ = np.array(B.rdd.collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfSample.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictX[dfSample.columns[5]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ZZ[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.array(A.filter(col(\"summary\") == \"count\").rdd.take(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "h = dfSample.columns\n",
    "for i, x in enumerate(dfSample.columns):\n",
    "    h[i] = dictX[x]\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(dfSample.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "h = ZZ[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "totalRDDrows = np.array(h[:-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print dfSample.rdd.getStorageLevel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try :\n",
    "    sourceDF\n",
    "except NameError:\n",
    "    pass\n",
    "else:\n",
    "    \"SourceDF already assigned, unpersist to force GC on dependencies \"\n",
    "    sourceDF.unpersist()\n",
    "    \n",
    "h_ = dfSample.columns\n",
    "sourceDF = dfSample.fillna(-1, subset=h_[:-1]).dropna(subset=h_[-1])\n",
    "sourceDF = sourceDF.persist(StorageLevel.MEMORY_AND_DISK)\n",
    "\n",
    "sourceDF_cover = dfSample.fillna(-1, subset=h_[:-1]).fillna(0, subset=h_[-1])\n",
    "sourceDF_cover = sourceDF.persist(StorageLevel.MEMORY_AND_DISK)\n",
    "\n",
    "\n",
    "h_ = sourceDF_cover.columns  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
