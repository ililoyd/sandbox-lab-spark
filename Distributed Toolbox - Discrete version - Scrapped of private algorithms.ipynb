{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Into HDFS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Source_Dir_X = \"src/X_output/workfile/\"\n",
    "HDFS_Dir_X = \"/user/cluster/AdLearn/Dev/workfile2/\"\n",
    "Source_Dir_Y = \"src/Y_output/\"\n",
    "HDFS_Dir_Y =\"/user/cluster/AdLearn/Dev/Y2/\"\n",
    "\n",
    "#import os\n",
    "print \"############################################################################\"\n",
    "print \"##### Commandes à input dans un shell d'une machine connectée à Hadoop #####\"\n",
    "print \"############################################################################\"\n",
    "print \"\" \n",
    "\n",
    "print 'if $(! hdfs dfs -test -d ' + HDFS_Dir_X + ') ; then hdfs dfs -mkdir ' + HDFS_Dir_X + '; fi'\n",
    "print 'if $(! hdfs dfs -test -d ' + HDFS_Dir_Y + ') ; then hdfs dfs -mkdir ' + HDFS_Dir_Y + '; fi'\n",
    "\n",
    "print \"\"\n",
    "print \"hdfs dfs -put -f \" + Source_Dir_X +\"*.csv \" + HDFS_Dir_X \n",
    "print \"hdfs dfs -put -f \" + Source_Dir_Y +\"*.csv \" + HDFS_Dir_Y \n",
    "\n",
    "#os.system(\"ls\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Configuration Zone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CLUSTER_ADD = \"hdfs://ad-cluster-master:54310\"\n",
    "SOURCE_DIR = \"/user/cluster/AdLearn/Debug/17-05-18/\"\n",
    "MODEL_DIR = \"/user/cluster/AdLearn/Debug/\"\n",
    "Y_DIR = \"/user/cluster/AdLearn/Debug/17-05-18/\"\n",
    "PARQUET_SOURCE_FILE = \"AdDF-debug12.parquet\"\n",
    "PARQUET_SOURCE_DIR = \"/user/cluster/AdLearn/Debug/\"\n",
    "Y_FILE = \"EW_YF3MCN.csv\"\n",
    "\n",
    "num_cores = 18\n",
    "gran = 2\n",
    "Force_regen = False\n",
    "\n",
    "BaseNameMergedFile = \"merged_forHDFS2\"\n",
    "Const_HEADER = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "import time\n",
    "import sys\n",
    "import random\n",
    "import math as m\n",
    "import functools\n",
    "from itertools import islice\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from pyspark.mllib.linalg import Vector, Vectors\n",
    "from pyspark.sql.functions import col, count, sum\n",
    "from pyspark.mllib.stat import Statistics\n",
    "from pyspark.mllib.linalg.distributed import *\n",
    "\n",
    "from hdfs import Config"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Aggregation into HDFS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def multipleJoins(x,y):\n",
    "    if isinstance(x, list) and isinstance(y, list):\n",
    "        return sorted(x + y)\n",
    "    elif isinstance(x, list):\n",
    "        return sorted(x + [y])\n",
    "    elif isinstance(y, list):\n",
    "        return sorted([x] + y)\n",
    "    else:\n",
    "        return sorted([x,y])\n",
    "\n",
    "try:\n",
    "    Const_HEADER\n",
    "except NameError:\n",
    "    sys.exit(\"Loading Configuration is needed !\")\n",
    "    \n",
    "client = Config().get_client()\n",
    "Parquet_List = client.list(PARQUET_SOURCE_DIR)\n",
    "regen = False if (PARQUET_SOURCE_FILE in Parquet_List) else True\n",
    "\n",
    "\n",
    "if regen == True or Force_regen == True :\n",
    "    t1 = time.time()\n",
    "    print \"##########################\"\n",
    "    print \"#### TASK: Files gathering\"\n",
    "    print \"Model Loading\"\n",
    "    rdd = sc.textFile(CLUSTER_ADD + MODEL_DIR + 'model.csv')\n",
    "    rddDate = rdd.zipWithIndex().filter(lambda ind: ind[1] > 0).map(lambda (x,y): (y,x)).map(lambda x: (x[0],x[1].split(\",\")[0]))\n",
    "    rddAction = rdd.zipWithIndex().filter(lambda ind: ind[1] > 0).map(lambda (x,y): (y,x)).map(lambda x: (x[0],x[1].split(\",\")[1]))\n",
    "\n",
    "    I = [rddDate, rddAction]\n",
    "    header = [\"Date\", \"Ids\"]\n",
    "\n",
    "    n = rddDate.count() + 1\n",
    "\n",
    "    print \"X vars Loading\"\n",
    "    rddFile = sc.textFile(CLUSTER_ADD + SOURCE_DIR + BaseNameMergedFile + '_X.csv', (gran+1) * num_cores)\n",
    "    rddX_ = rddFile.map(lambda x: np.nan if x=='\"\"' else x).zipWithIndex().map(lambda x: (x[1] % n , x[0]))\n",
    "    rddX  = rddX_.filter(lambda ind: ind[0] > 0).map(lambda (x,y): (x,float(y)))\n",
    "\n",
    "    header_ = sc.textFile(CLUSTER_ADD + SOURCE_DIR + BaseNameMergedFile + '_X_header.csv').collect()\n",
    "\n",
    "    header = header + header_\n",
    "\n",
    "    I.append(rddX)\n",
    "\n",
    "    print \"Y vars Loading\"\n",
    "    rddFile = sc.textFile(CLUSTER_ADD + Y_DIR + BaseNameMergedFile +'_Y.csv')\n",
    "\n",
    "    rddY_ = rddFile.map(lambda x: np.nan if x=='\"\"' else x).zipWithIndex().map(lambda x: (x[1] % n , x[0]))\n",
    "    rddY  = rddY_.filter(lambda ind: ind[0] > 0).map(lambda (x,y): (x,float(y)))\n",
    "\n",
    "    header_ = sc.textFile(CLUSTER_ADD + Y_DIR  + BaseNameMergedFile + '_Y_header.csv').collect()\n",
    "\n",
    "    header = header + header_\n",
    "\n",
    "    I.append(rddY)\n",
    "    \n",
    "    print(\"#############################\")\n",
    "    print(\"TASK: RDD to DF:\")\n",
    "    \n",
    "    uni2 =  sc.union(I).zipWithIndex().map(lambda ind: [ind[0][0],(ind[1], ind[0][1])]).coalesce((gran) * num_cores)\n",
    "\n",
    "    s = uni2.reduceByKey(multipleJoins).map(lambda x: x[1]).map(lambda tuples: [x[1] for x in tuples])\n",
    "\n",
    "    df = s.toDF(header)\n",
    "    \n",
    "    print(\"#############################\")\n",
    "    print(\"TASK: Save Aggregated to HDFS:\")\n",
    "    df.write.parquet( CLUSTER_ADD + PARQUET_SOURCE_DIR + PARQUET_SOURCE_FILE)\n",
    "\n",
    "print \"FILE AGGREGATION FINISHED\"\n",
    "t2 = time.time()\n",
    "print(\"####################################\")\n",
    "print(\"TIME TAKEN FOR TASK:\" + str(t2-t1))\n",
    "#check_integrity(df,FilesList)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Integrity Test (non retested for new Aggregation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#check_integrity\n",
    "client = Config().get_client()\n",
    "FilesList = client.list(SOURCE_DIR)\n",
    "header = df2.columns\n",
    "seed = time.time()\n",
    "random.seed(seed)\n",
    "num_check = int(len(FilesList) * 1 / 10)\n",
    "\n",
    "print(num_check)\n",
    "\n",
    "for i in xrange(len(FilesList)):\n",
    "    id = random.randint(0,len(FilesList) - 1)\n",
    "    print(\"WORK: \" + str(i) + \" ; \" + str(id))\n",
    "    \n",
    "    with client.read(SOURCE_DIR + FilesList[i]) as reader:\n",
    "        dfPandas = pd.read_csv(reader)\n",
    "    \n",
    "    dfCheck = df2.orderBy(\"Date\", \"Ids\").select([header[2+i]]).toPandas()\n",
    "    assert dfPandas.equals(dfCheck), \"Integrity Check Failure: \" + str(header[2+i]) + \\\n",
    "                                                \"id: \" + str(2+i) +\" seed: \" + str(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading Data from HDFS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"LOADING PARQUET FILE DATAFRAME\")\n",
    "t1 = time.time()\n",
    "df = spark.read.parquet( CLUSTER_ADD + PARQUET_SOURCE_DIR + PARQUET_SOURCE_FILE)\n",
    "t2 = time.time()\n",
    "print(\"####################################\")\n",
    "print(\"TIME TAKEN FOR TASK:\" + str(t2-t1))\n",
    "\n",
    "print(\"#############################\")\n",
    "print(\"Parquet loading TASK FINISHED\")\n",
    "print(\"\\n\")\n",
    "    \n",
    "print(\"Dataframe LOADED\")\n",
    "\n",
    "#dfPersist = df.persist(StorageLevel.MEMORY_AND_DISK)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sampling Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ChosenVar = df.columns.index(\"EWYF3MCN\")\n",
    "\n",
    "h_ = df.columns[2:-4]\n",
    "h_.append(df.columns[ChosenVar])\n",
    "h_\n",
    "\n",
    "dateDebut = df.select(\"Date\",h_[-1]).orderBy(\"Date\").dropna().select(\"Date\").limit(1).rdd.map(lambda x: list(x)[0]).take(1)[0]\n",
    "\n",
    "\n",
    "dfSample = df.filter(col(\"Date\") >= dateDebut).filter(col(\"Date\") < '2016-01-01')\\\n",
    "                .orderBy(\"Date\",\"Ids\").select(h_).persist()\n",
    "    \n",
    "dY = dfSample.select(h_[-1])\n",
    "\n",
    "dY.show()\n",
    "dfSample.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.select(\"Date\").orderBy(\"Date\", ascending = False).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def isPos(x):\n",
    "    if x > 0:\n",
    "        return 1\n",
    "    if x < 0:\n",
    "        return -1\n",
    "    return np.nan\n",
    "\n",
    "\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "udf_discretize = udf(isPos, IntegerType())\n",
    "dY_log = dY.withColumn(\"Y_log\", udf_discretize(h_[-1])).select(\"Y_log\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compute ExpY and MeanY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t1 = time.time()\n",
    "t = dY.filter(dY[dfSample.columns[-1]] != np.nan).rdd.map(lambda data: Vectors.dense([float(c) for c in data]))\n",
    "b = Statistics.colStats(t)\n",
    "Ecart_Y = m.sqrt(b.variance())\n",
    "Esp_Y = b.mean()[0]\n",
    "t2 = time.time()\n",
    "\n",
    "print(t2-t1)\n",
    "print(Ecart_Y)\n",
    "print(Esp_Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t1 = time.time()\n",
    "t = dY_log.filter(dY_log[\"Y_log\"] != np.nan).rdd.map(lambda data: Vectors.dense([float(c) for c in data]))\n",
    "b = Statistics.colStats(t)\n",
    "Ecart_Y = m.sqrt(b.variance())\n",
    "Esp_Y = b.mean()[0]\n",
    "t2 = time.time()\n",
    "\n",
    "print(t2-t1)\n",
    "print(Ecart_Y)\n",
    "print(Esp_Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfSample = dfSample.withColumn(\"Y_log\", udf_discretize(h_[-1])).select(h_[:-1] + [\"Y_log\"]).persist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Distributed Toolbox"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# take 'rdd' of n * p size with n - 1 features vector and 1 y vector\n",
    "# take 2 extrema for comparison\n",
    "# Takes out y vector\n",
    "# Return activation vector by comparing with the extrema\n",
    "def map_booleanRuling(x, min_b, max_b):\n",
    "    row = np.array(x[:-1])\n",
    "    bool_ = np.logical_and(min_b <= row, row <= max_b).astype(int)\n",
    "    \n",
    "    return np.append(bool_, x[-1])\n",
    "\n",
    "# Vectoriel sum\n",
    "# \"Vertical sum\" in terms of rdd\n",
    "# Last step of \"internal\" Dot Product\n",
    "def red_vectSum(x,y):   \n",
    "    return np.array(x) + np.array(y)\n",
    "\n",
    "def red_vectSum_withoutLast(x,y):\n",
    "    return red_vectSum(x[:-1],y[:-1])\n",
    "             \n",
    "# Take rdd 'x' and boolean selector of size n - 1.\n",
    "# x is of size n * p is n - 1 features vectors + 1 y vector (last column)\n",
    "# return the m selected vectors of 'x' and y vector in one rdd\n",
    "def map_selectActiv(x,boolCol):\n",
    "    y = np.array(x[:-1])[boolCol]\n",
    "    \n",
    "    return np.append(y,x[-1]).tolist()\n",
    "\n",
    "# take rdd 'x' of size n * p with n - 1 features vectors and 1 y vector\n",
    "# return rdd with each x_i,j element multiplied by associated y_i \n",
    "# First step of \"internal\" Dot Product\n",
    "# Erase y_i column in returned rdd\n",
    "def map_Internal_YProduct(x):\n",
    "\n",
    "    return (np.array(x[:-1]) * x[-1])\n",
    "\n",
    "# FACADE function (design pattern)\n",
    "# wrapping up all error function adapted for the current learning\n",
    "def computeError(rdd, esp_cond, nb_act, function = \"MSE\"):\n",
    "    if function == \"MSE\":\n",
    "        \n",
    "        return computeMSE(rdd, esp_cond, nb_act)\n",
    "    else:\n",
    "        \n",
    "        raise Exception(\"No valid error function provided !\")\n",
    "\n",
    "        \n",
    "def map_MSE(x, esp):\n",
    "    exp = (esp - x[-1]) ** 2\n",
    "    x = np.array(x[:-1])\n",
    "    return np.multiply(x, exp) \n",
    "        \n",
    "    \n",
    "# MSE loss function\n",
    "def computeMSE(rdd, esp_cond, nb_act):\n",
    "    esp_cond = np.array(esp_cond)\n",
    "    boolTest = esp_cond >= 0\n",
    "    esp_cond = np.where(boolTest, 1, -1)\n",
    "    rdd = rdd.map(functools.partial(map_MSE, esp=esp_cond))\n",
    "    \n",
    "    rdd = rdd.reduce(red_vectSum)\n",
    "    array = np.true_divide(np.array(rdd), nb_act)\n",
    "    \n",
    "    return array\n",
    "\n",
    "def computeCondProb(rdd, nb_act):\n",
    "    #Map = Matrix of (x_i,j * y_i)\n",
    "    #Reduce = Vertical Sum\n",
    "    # Map + Reduce = X . Y_transpose\n",
    "    dotProduct = rdd.map(map_Internal_YProduct).reduce(red_vectSum)\n",
    "    \n",
    "    # X.Y / nb_activation sur les règles qui passent le filtre de coverage\n",
    "    return np.true_divide(np.array(dotProduct), nb_act)\n",
    "    \n",
    "def computeZscore(prob_cond, nb_act, esp_y, ecart_y):\n",
    "    deltaCond = np.absolute(prob_cond - esp_y)\n",
    "    \n",
    "    # | Esp_Cond_Y - Esp_Y | / (Ecart_Y / Racine(nb_activation)) \n",
    "    Zscore = np.multiply(deltaCond, np.sqrt(nb_act) / ecart_y)\n",
    "    return Zscore\n",
    "        \n",
    "def red_vectSum_withoutY(x,y):\n",
    "    return red_vectSum([x[0]], [y[0]])\n",
    "\n",
    "# take 'rdd' of n * p size with n - 1 features vector and 1 y vector\n",
    "# take 2 extrema for comparison\n",
    "# Takes out y vector\n",
    "# Return activation vector by comparing with the extrema\n",
    "def map_booleanRuling_list(x, min_b, max_b):\n",
    "    row = np.array(x[:-1])\n",
    "    bool_test = np.logical_and(row >= np.array(min_b), row <= np.array(max_b))\n",
    "    \n",
    "    return np.append(np.where(bool_test,1,0), x[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "B = dfSample.na.replace(np.nan, 'NA').agg(*[\n",
    "    count(c).alias(c)    # vertical operations in SQL ignore NULL\n",
    "    for c in dfSample.columns\n",
    "])\n",
    "temp = np.array(B.rdd.collect())\n",
    "h = temp[0]\n",
    "totalRDDrows = np.array(h[:-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "totalRDDrows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try :\n",
    "    sourceDF\n",
    "except NameError:\n",
    "    pass\n",
    "else:\n",
    "    \"SourceDF already assigned, unpersist to force GC on dependencies \"\n",
    "    sourceDF.unpersist()\n",
    "    \n",
    "h_ = dfSample.columns\n",
    "sourceDF = dfSample.fillna(-1, subset=h_[:-1]).dropna(subset=h_[-1])\n",
    "sourceDF = sourceDF.persist(StorageLevel.MEMORY_AND_DISK)\n",
    "\n",
    "sourceDF_cover = dfSample.fillna(-1, subset=h_[:-1]).fillna(0, subset=h_[-1])\n",
    "sourceDF_cover = sourceDF.persist(StorageLevel.MEMORY_AND_DISK)\n",
    "\n",
    "\n",
    "h_ = sourceDF_cover.columns  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
