{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = pd.read_csv(\"X/Esg_ANAll!Disclosure_Score_M10.csv\", index_col=0)\n",
    "test = test.fillna(0)\n",
    "test\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c = test.columns.values\n",
    "c\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c.size\n",
    "d.size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "e = np.unique(d)\n",
    "e.size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = np.append(d,'IZ389654')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "h = np.unique(g)\n",
    "h.size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "h = np.delete(h, 0)\n",
    "np.sort(h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "########## SCAN AND CONSTRUCT MATRIX FORM ######################\n",
    "filesList = glob.glob(\"X/*.csv\")\n",
    "\n",
    "ix = []\n",
    "col = []\n",
    "indexList = np.array(ix)\n",
    "colList = np.array(col)\n",
    "\n",
    "for f_ in filesList:\n",
    "    print(f_)\n",
    "    df = pd.read_csv(f_, index_col=0)\n",
    "    indexList = np.append(indexList, df.index.values)\n",
    "    indexList = np.unique(indexList)\n",
    "    indexList = np.sort(indexList)    \n",
    "    \n",
    "    colList = np.append(colList, df.columns.values)\n",
    "    colList = np.unique(colList)\n",
    "    colList = np.sort(colList)\n",
    "    \n",
    "##### Rustine pour drop CIQID\n",
    "colList = np.delete(colList,0)\n",
    "##############################\n",
    "\n",
    "## Generate the DF for double index stacking\n",
    "df = pd.DataFrame(index=indexList,columns=colList, data=0)\n",
    "temp = df.stack()\n",
    "\n",
    "## To drop the unwanted 0 column\n",
    "temp.to_csv(\"X_output/_model.csv\")\n",
    "df = pd.read_csv(\"X_output/_model.csv\")\n",
    "\n",
    "T = df.drop(\"0\", axis=1)\n",
    "\n",
    "#FINAL MODEL\n",
    "T.to_csv(\"X_output/model.csv\", index=False)   \n",
    "os.remove(\"X_output/_model.csv\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def fill_in(model,df_wide):\n",
    "    index = df_wide.index.values\n",
    "    \n",
    "    df = model\n",
    "    df['value'] = 0\n",
    "    \n",
    "    for i in range(len(index)):\n",
    "        a = df_wide.index.values[i]\n",
    "        print(a)\n",
    "        print(i)\n",
    "        df.ix[a].value = df_wide.ix[a].values\n",
    "        \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import glob\n",
    "\n",
    "\n",
    "filesList = glob.glob(\"X/*.csv\")\n",
    "\n",
    "total = len(filesList)\n",
    "i = 0\n",
    "for f_ in filesList:\n",
    "    i = i + 1\n",
    "    print(\"Work: \" + str(i) + \"/\" + str(total))\n",
    "    print(f_)\n",
    "    f_c = f_.replace('.csv','')\n",
    "    f_c = f_c.replace('X/','')\n",
    "    f_c_ = ''.join(e for e in f_c if e.isalnum())\n",
    "\n",
    "\n",
    "    test = pd.read_csv(f_, index_col=0)\n",
    "    if 'CIQID' in test.columns:\n",
    "        del test['CIQID']\n",
    "    \n",
    "    test = test.fillna(0)\n",
    "    test = test.stack().to_frame()\n",
    "\n",
    "    model = pd.read_csv(\"X_output/model.csv\", index_col=[0,1])\n",
    "\n",
    "    df = pd.DataFrame(index=model.index.difference(test.index),data=0,columns=[0])\n",
    "\n",
    "    frames = [test, df]\n",
    "    bDF = pd.concat(frames)\n",
    "    bDF.sort_index()\n",
    "    bDF.columns = [f_c_]\n",
    "    bDF.to_csv(\"X_output/standalone/\"+f_c+\".csv\",index=True)\n",
    "    bDF.to_csv(\"X_output/workfile/\"+f_c+\".csv\",index=False)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.index.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(model.index.difference(test.index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(model.index.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(test.index.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = spark.read.format(\"csv\")\\\n",
    "        .option(\"header\", \"true\")\\\n",
    "        .option(\"mode\", \"PERMISSIVE\")\\\n",
    "        .option(\"inferSchema\", \"true\")\\\n",
    "        .option(\"delimiter\", \",\")\\\n",
    "        .csv(\"hdfs://ad-cluster-master:54310/user/cluster/TestCorrel/workA.csv\")\n",
    "df1.show()\n",
    "\n",
    "df2 = spark.read.format(\"csv\")\\\n",
    "        .option(\"header\", \"true\")\\\n",
    "        .option(\"mode\", \"PERMISSIVE\")\\\n",
    "        .option(\"inferSchema\", \"true\")\\\n",
    "        .option(\"delimiter\", \",\")\\\n",
    "        .csv(\"hdfs://ad-cluster-master:54310/user/cluster/TestCorrel/workB.csv\")\n",
    "df2.show()\n",
    "\n",
    "df3 = spark.read.format(\"csv\")\\\n",
    "        .option(\"header\", \"true\")\\\n",
    "        .option(\"mode\", \"PERMISSIVE\")\\\n",
    "        .option(\"inferSchema\", \"true\")\\\n",
    "        .option(\"delimiter\", \",\")\\\n",
    "        .csv(\"hdfs://ad-cluster-master:54310/user/cluster/TestCorrel/standA.csv\")\n",
    "df3.show()\n",
    "\n",
    "df4 = spark.read.format(\"csv\")\\\n",
    "        .option(\"header\", \"true\")\\\n",
    "        .option(\"mode\", \"PERMISSIVE\")\\\n",
    "        .option(\"inferSchema\", \"true\")\\\n",
    "        .option(\"delimiter\", \",\")\\\n",
    "        .csv(\"hdfs://ad-cluster-master:54310/user/cluster/TestCorrel/standB.csv\")\n",
    "df4.show()\n",
    "\n",
    "df3 = df3.withColumnRenamed('_c0','date')\n",
    "df3 = df3.withColumnRenamed('_c1','Action')\n",
    "df4 = df4.withColumnRenamed('_c0','ix0')\n",
    "df4 = df4.withColumnRenamed('_c1','ix1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df3.createOrReplaceTempView(\"d3\")\n",
    "df4.createOrReplaceTempView(\"d4\")\n",
    "\n",
    "keep = df3.columns[0] + \",\" + df3.columns[1] + \",\" + df3.columns[2] + \",\" + df4.columns[2]\n",
    "\n",
    "query = \"SELECT {} FROM d3 d LEFT JOIN d4 e ON d.date = e.ix0 AND d.Action = e.ix1\".format(keep)\n",
    "print(query)\n",
    "\n",
    "sqlContext.sql(query).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df3 = spark.read.format(\"csv\")\\\n",
    "        .option(\"header\", \"true\")\\\n",
    "        .option(\"mode\", \"PERMISSIVE\")\\\n",
    "        .option(\"inferSchema\", \"true\")\\\n",
    "        .option(\"delimiter\", \",\")\\\n",
    "        .csv(\"hdfs://ad-cluster-master:54310/user/cluster/TestCorrel/standA.csv\")\n",
    "df3.show()\n",
    "\n",
    "df4 = spark.read.format(\"csv\")\\\n",
    "        .option(\"header\", \"true\")\\\n",
    "        .option(\"mode\", \"PERMISSIVE\")\\\n",
    "        .option(\"inferSchema\", \"true\")\\\n",
    "        .option(\"delimiter\", \",\")\\\n",
    "        .csv(\"hdfs://ad-cluster-master:54310/user/cluster/TestCorrel/standB.csv\")\n",
    "df4.show()\n",
    "\n",
    "df3 = df3.withColumnRenamed('_c0','date')\n",
    "df3 = df3.withColumnRenamed('_c1','Action')\n",
    "df4 = df4.withColumnRenamed('_c0','ix0')\n",
    "df4 = df4.withColumnRenamed('_c1','ix1')\n",
    "\n",
    "df3.createOrReplaceTempView(\"d3\")\n",
    "df4.createOrReplaceTempView(\"d4\")\n",
    "\n",
    "keep = df3.columns[0] + \",\" + df3.columns[1] + \",\" + df3.columns[2] + \",\" + df4.columns[2]\n",
    "\n",
    "query = \"SELECT {} FROM d3 d LEFT JOIN d4 e ON d.date = e.ix0 AND d.Action = e.ix1\".format(keep)\n",
    "\n",
    "df7 = sqlContext.sql(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df7 = sqlContext.sql(query)\n",
    "\n",
    "df7.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from hdfs import Config\n",
    "\n",
    "######## CONFIG #######################\"\"\n",
    "SOURCE_DIR = \"hdfs://ad-cluster-master:54310/user/cluster/TestCorrel/source/\"\n",
    "\n",
    "\n",
    "\n",
    "client = Config().get_client()\n",
    "FilesList = client.list('/user/cluster/TestCorrel/source')\n",
    "total = len(FilesList)\n",
    "\n",
    "first = FilesList.pop(0)\n",
    "\n",
    "#### INIT ##################################\n",
    "dfW = spark.read.format(\"csv\")\\\n",
    "    .option(\"header\", \"true\")\\\n",
    "    .option(\"mode\", \"PERMISSIVE\")\\\n",
    "    .option(\"inferSchema\", \"true\")\\\n",
    "    .option(\"delimiter\", \",\")\\\n",
    "    .csv(SOURCE_DIR + first)\n",
    "    \n",
    "dfW = dfW.withColumnRenamed('_c0','date')\n",
    "dfW = dfW.withColumnRenamed('_c1','Action')\n",
    "i=0\n",
    "\n",
    "for file_ in FilesList:\n",
    "    i=i+1\n",
    "    print(\"####################################\")\n",
    "    print(\"Work: \" + str(i) + \"/\" + str(total))\n",
    "    print(\"####################################\")\n",
    "    print(file_)\n",
    "\n",
    "    dftemp = spark.read.format(\"csv\")\\\n",
    "            .option(\"header\", \"true\")\\\n",
    "            .option(\"mode\", \"PERMISSIVE\")\\\n",
    "            .option(\"inferSchema\", \"true\")\\\n",
    "            .option(\"delimiter\", \",\")\\\n",
    "            .csv(SOURCE_DIR + file_)\n",
    "\n",
    "    \n",
    "    dftemp = dftemp.withColumnRenamed('_c0','ix0')\n",
    "    dftemp = dftemp.withColumnRenamed('_c1','ix1')\n",
    "\n",
    "    dfW.createOrReplaceTempView(\"fW\")\n",
    "    dftemp.createOrReplaceTempView(\"ftemp\")\n",
    "\n",
    "    keep = ','.join(map(str, dfW.columns))  + \",\" + dftemp.columns[2]\n",
    "\n",
    "    query = \"SELECT {} FROM fW d LEFT JOIN ftemp e ON d.date = e.ix0 AND d.Action = e.ix1\".format(keep)\n",
    "\n",
    "    \n",
    "    dfW = sqlContext.sql(query)\n",
    "\n",
    "    \n",
    "##### STORAGE\n",
    "dfW.write\\\n",
    "    .format(\"csv\")\\\n",
    "    .option(\"header\", \"true\")\\\n",
    "    .save(\"hdfs://ad-cluster-master:54310/user/cluster/TestCorrel/_BigMatrix__.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df7.coalesce(1)\\\n",
    "    .write\\\n",
    "    .format(\"csv\")\\\n",
    "    .option(\"header\", \"true\")\\\n",
    "    .save(\"./output2.csv\")\\\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfW.write\\\n",
    "    .format(\"csv\")\\\n",
    "    .option(\"header\", \"true\")\\\n",
    "    .save(\"hdfs://ad-cluster-master:54310/user/cluster/TestCorrel/pBigM_.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfW.coalesce(1)\\\n",
    "    .write\\\n",
    "    .format(\"csv\")\\\n",
    "    .option(\"header\", \"true\")\\\n",
    "    .save(\"hdfs://ad-cluster-master:54310/user/cluster/TestCorrel/_Big.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfW.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.mllib.stat import Statistics\n",
    "\n",
    "L_df = dfW.columns\n",
    "a = dfW.stat.corr(L_df[2],L_df[3])\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfW.repartition(64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from hdfs import Config\n",
    "\n",
    "######## CONFIG #######################\"\"\n",
    "SOURCE_DIR = \"hdfs://ad-cluster-master:54310/user/cluster/TestCorrel/source/\"\n",
    "\n",
    "\n",
    "\n",
    "client = Config().get_client()\n",
    "FilesList = client.list('/user/cluster/TestCorrel/source')\n",
    "FilesList = FilesList[:100]\n",
    "total = len(FilesList)\n",
    "\n",
    "first = FilesList.pop(0)\n",
    "\n",
    "#### INIT ##################################\n",
    "dfW = spark.read.format(\"csv\")\\\n",
    "    .option(\"header\", \"true\")\\\n",
    "    .option(\"mode\", \"PERMISSIVE\")\\\n",
    "    .option(\"inferSchema\", \"true\")\\\n",
    "    .option(\"delimiter\", \",\")\\\n",
    "    .csv(SOURCE_DIR + first)\n",
    "    \n",
    "dfW = dfW.withColumnRenamed('_c0','date')\n",
    "dfW = dfW.withColumnRenamed('_c1','Action')\n",
    "i=0\n",
    "for file_ in FilesList:\n",
    "    i=i+1\n",
    "    print(\"####################################\")\n",
    "    print(\"Work: \" + str(i) + \"/\" + str(total))\n",
    "    print(\"####################################\")\n",
    "    print(file_)\n",
    "\n",
    "    dftemp = spark.read.format(\"csv\")\\\n",
    "            .option(\"header\", \"true\")\\\n",
    "            .option(\"mode\", \"PERMISSIVE\")\\\n",
    "            .option(\"inferSchema\", \"true\")\\\n",
    "            .option(\"delimiter\", \",\")\\\n",
    "            .csv(SOURCE_DIR + file_)\n",
    "\n",
    "    \n",
    "    dftemp = dftemp.withColumnRenamed('_c0','ix0')\n",
    "    dftemp = dftemp.withColumnRenamed('_c1','ix1')\n",
    "\n",
    "    dfW.createOrReplaceTempView(\"fW\")\n",
    "    dftemp.createOrReplaceTempView(\"ftemp\")\n",
    "\n",
    "    keep = ','.join(map(str, dfW.columns))  + \",\" + dftemp.columns[2]\n",
    "\n",
    "    query = \"SELECT {} FROM fW d LEFT JOIN ftemp e ON d.date = e.ix0 AND d.Action = e.ix1\".format(keep)\n",
    "\n",
    "    \n",
    "    dfW = sqlContext.sql(query)\n",
    "\n",
    "    \n",
    "'''\n",
    "dfW.write\\\n",
    "    .format(\"csv\")\\\n",
    "    .option(\"header\", \"true\")\\\n",
    "    .save(\"hdfs://ad-cluster-master:54310/user/cluster/TestCorrel/_BigMatrix__.csv\")\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.mllib.stat import Statistics\n",
    "\n",
    "dfW.repartition(64)\n",
    "L_df = dfW.columns\n",
    "a = dfW.stat.corr(L_df[2],L_df[3])\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dfSave = dfW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from hdfs import Config\n",
    "from pyspark.mllib.stat import Statistics\n",
    "import time\n",
    "######## CONFIG #######################\"\"\n",
    "SOURCE_DIR = \"hdfs://ad-cluster-master:54310/user/cluster/TestCorrel/source/\"\n",
    "\n",
    "num_cores = 16\n",
    "\n",
    "t1 = time.time()\n",
    "client = Config().get_client()\n",
    "FilesList = client.list('/user/cluster/TestCorrel/source')\n",
    "FilesList = FilesList[:50]\n",
    "total = len(FilesList)\n",
    "\n",
    "first = FilesList.pop(0)\n",
    "\n",
    "#### INIT ##################################\n",
    "dfW = spark.read.format(\"csv\")\\\n",
    "    .option(\"header\", \"true\")\\\n",
    "    .option(\"mode\", \"PERMISSIVE\")\\\n",
    "    .option(\"inferSchema\", \"true\")\\\n",
    "    .option(\"delimiter\", \",\")\\\n",
    "    .csv(SOURCE_DIR + first)\n",
    "    \n",
    "dfW = dfW.withColumnRenamed('_c0','date')\n",
    "dfW = dfW.withColumnRenamed('_c1','Action')\n",
    "i=0\n",
    "\n",
    "dfW.repartition(7*num_cores)\n",
    "for file_ in FilesList:\n",
    "    i=i+1\n",
    "    print(\"####################################\")\n",
    "    print(\"Work: \" + str(i) + \"/\" + str(total))\n",
    "\n",
    "    dftemp = spark.read.format(\"csv\")\\\n",
    "            .option(\"header\", \"true\")\\\n",
    "            .option(\"mode\", \"PERMISSIVE\")\\\n",
    "            .option(\"inferSchema\", \"true\")\\\n",
    "            .option(\"delimiter\", \",\")\\\n",
    "            .csv(SOURCE_DIR + file_)\n",
    "\n",
    "    \n",
    "    dftemp = dftemp.withColumnRenamed('_c0','ix0')\n",
    "    dftemp = dftemp.withColumnRenamed('_c1','ix1')\n",
    "\n",
    "    dfW.createOrReplaceTempView(\"fW\")\n",
    "    dftemp.createOrReplaceTempView(\"ftemp\")\n",
    "\n",
    "    keep = ','.join(map(str, dfW.columns))  + \",\" + dftemp.columns[2]\n",
    "\n",
    "    query = \"SELECT {} FROM fW d LEFT JOIN ftemp e ON d.date = e.ix0 AND d.Action = e.ix1\".format(keep)\n",
    "\n",
    "    \n",
    "    dfW = sqlContext.sql(query)\n",
    "    dfW.repartition(7*num_cores)\n",
    "    if (i % 10 == 0):\n",
    "        dfW.persist(StorageLevel.DISK_ONLY)\n",
    "        \n",
    "    \n",
    "'''\n",
    "dfW.write\\\n",
    "    .format(\"csv\")\\\n",
    "    .option(\"header\", \"true\")\\\n",
    "    .save(\"hdfs://ad-cluster-master:54310/user/cluster/TestCorrel/_BigMatrix__.csv\")\n",
    "\n",
    "\n",
    "\n",
    "L_df = dfW.columns\n",
    "a = dfW.stat.corr(L_df[2],L_df[3])\n",
    "print(a)\n",
    "\n",
    "dfW.cache()\n",
    "b = dfW.stat.corr(L_df[2],L_df[8])\n",
    "print(b)\n",
    "'''\n",
    "t2 = time.time()\n",
    "print(\"####################################\")\n",
    "print(t2-t1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t1 = time.time()\n",
    "dfW.cache()\n",
    "t2 = time.time()\n",
    "print(t2-t1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "t1 = time.time()\n",
    "L_df = dfW.columns\n",
    "s = len(L_df)\n",
    "t = s - 2\n",
    "\n",
    "Mat_cor = np.ones((t,t))\n",
    "\n",
    "\n",
    "for i in range(t):\n",
    "    for j in range(i+1,t):\n",
    "        print(\"i:\"+str(i)+\" j:\"+str(j))\n",
    "        Mat_cor[i,j] = dfW.stat.corr(L_df[i + 2], L_df[j+2])\n",
    "        Mat_cor[j,i] = Mat_cor[i,j]\n",
    "        print(\"correl: \"+str(Mat_cor[i,j]))\n",
    "        \n",
    "t2 = time.time()\n",
    "print(t2-t1)\n",
    "Mat_cor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "L_df = dfW.columns\n",
    "colsRows = L_df[2:]\n",
    "\n",
    "dfSave =dfW\n",
    "\n",
    "pd_df = pd.DataFrame(data=Mat_cor, columns=colsRows, index=colsRows)\n",
    "\n",
    "pd_df.to_csv(\"./testCor.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dfSave =dfW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "dfSave.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfSave.cache()\n",
    "b = dfW.stat.corr(L_df[2],L_df[8])\n",
    "print(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b = dfW.stat.corr(L_df[3],L_df[82])\n",
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfW.rdd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfW.coalesce(1).write\\\n",
    "    .format(\"csv\")\\\n",
    "    .option(\"header\", \"true\")\\\n",
    "    .save(\"hdfs://ad-cluster-master:54310/user/cluster/foo_export.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from hdfs import Config\n",
    "from pyspark.mllib.stat import Statistics\n",
    "import time\n",
    "######## CONFIG #######################\"\"\n",
    "SOURCE_DIR = \"hdfs://ad-cluster-master:54310/user/cluster/source_wfile/\"\n",
    "\n",
    "num_cores = 16\n",
    "\n",
    "t1 = time.time()\n",
    "client = Config().get_client()\n",
    "FilesList = client.list('/user/cluster/source_wfile')\n",
    "\n",
    "first = FilesList.pop(0)\n",
    "\n",
    "#### INIT ##################################\n",
    "dfW = spark.read.format(\"csv\")\\\n",
    "    .option(\"header\", \"true\")\\\n",
    "    .option(\"mode\", \"PERMISSIVE\")\\\n",
    "    .option(\"inferSchema\", \"true\")\\\n",
    "    .option(\"delimiter\", \",\")\\\n",
    "    .csv(SOURCE_DIR + first)\n",
    "    \n",
    "dfW.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfW.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfW.rdd.persist(StorageLevel.MEMORY_AND_DISK)\n",
    "L = []\n",
    "L.append(dfW.rdd)\n",
    "dfW.rdd.repartition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from hdfs import Config\n",
    "from pyspark.mllib.stat import Statistics\n",
    "import time\n",
    "######## CONFIG #######################\"\"\n",
    "SOURCE_DIR = \"hdfs://ad-cluster-master:54310/user/cluster/source_wfile/\"\n",
    "\n",
    "num_cores = 16\n",
    "\n",
    "t1 = time.time()\n",
    "client = Config().get_client()\n",
    "FilesList = client.list('/user/cluster/source_wfile')\n",
    "FilesList = FilesList[:10]\n",
    "total = len(FilesList)\n",
    "\n",
    "L=[]\n",
    "i=0\n",
    "for file_ in FilesList:\n",
    "    i=i+1\n",
    "    print(\"####################################\")\n",
    "    print(\"Work: \" + str(i) + \"/\" + str(total))\n",
    "\n",
    "    dftemp = spark.read.format(\"csv\")\\\n",
    "            .option(\"header\", \"true\")\\\n",
    "            .option(\"mode\", \"PERMISSIVE\")\\\n",
    "            .option(\"inferSchema\", \"true\")\\\n",
    "            .option(\"delimiter\", \",\")\\\n",
    "            .csv(SOURCE_DIR + file_)\n",
    "    \n",
    "    rddT = dftemp.rdd\n",
    "    #print(rddT)\n",
    "    #rddT = dftemp.rdd.map(lambda x: x[0])\n",
    "    #rddT = rddT.repartition(1)\n",
    "    rddT = rddT.repartition(3*num_cores)\n",
    "    #rddT.persist(StorageLevel.MEMORY_AND_DISK)\n",
    "    L.append(rddT)\n",
    "    \n",
    "    \n",
    "    '''dftemp.repartition(3*num_cores)\n",
    "    dftemp.persist(StorageLevel.MEMORY_AND_DISK)\n",
    "    P.append(dftemp)'''\n",
    "    \n",
    "len(L)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.mllib.stat import Statistics\n",
    "a = L[1].collect()\n",
    "b = L[2].collect()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "L[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = sc.parallelize(L[1])\n",
    "y = sc.parallelize(L[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c = L[1].map(list)\n",
    "e = L[2].map(list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = c.collect()\n",
    "f = e.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = L[1].map(tuple)\n",
    "g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "h = g.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seriesX = sc.parallelize([1.0, 2.0, 3.0, 3.0, 5.0])  # a series\n",
    "# seriesY must have the same number of partitions and cardinality as seriesX\n",
    "seriesY = sc.parallelize([11.0, 22.0, 33.0, 33.0, 555.0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seriesX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.mllib.stat import Statistics\n",
    "a = L[1]\n",
    "b = L[2]\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Statistics.corr(a,a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = sc.parallelize(a.map(list).collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rT = dftemp.rdd.map(lambda x: x[0])\n",
    "rT.persist(StorageLevel.MEMORY_AND_DISK)\n",
    "Statistics.corr(rT,rT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "L"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a=L[2]\n",
    "b=L[3]\n",
    "\n",
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Statistics.corr(x=L[5],y=L[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Statistics.corr(x=L[2],y=L[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Statistics.corr(x=L[4],y=L[8])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "t1 = time.time()\n",
    "L_rdd = L\n",
    "s = len(L_rdd)\n",
    "t = s\n",
    "\n",
    "Mat_cor = np.ones((t,t))\n",
    "\n",
    "\n",
    "for i in range(t):\n",
    "    for j in range(i+1,t):\n",
    "        print(\"i:\"+str(i)+\" j:\"+str(j))\n",
    "        Mat_cor[i,j] = Statistics.corr(L_rdd[i], L_rdd[j])\n",
    "        Mat_cor[j,i] = Mat_cor[i,j]\n",
    "        print(\"correl: \"+str(Mat_cor[i,j]))\n",
    "        \n",
    "t2 = time.time()\n",
    "print(t2-t1)\n",
    "Mat_cor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "a = L[1]\n",
    "b = L[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = L[1]\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "a.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b = a.partitionBy(100)\n",
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c = b.map(tuple)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "L[1].collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_in_a_partition(iterator):\n",
    "  yield sum(1 for _ in iterator)\n",
    "\n",
    "print(L[1].mapPartitions(count_in_a_partition).collect())\n",
    "print(L[2].mapPartitions(count_in_a_partition).collect())\n",
    "print(L[3].mapPartitions(count_in_a_partition).collect())\n",
    "print(L[4].mapPartitions(count_in_a_partition).collect())\n",
    "print(L[5].mapPartitions(count_in_a_partition).collect())\n",
    "print(L[6].mapPartitions(count_in_a_partition).collect())\n",
    "print(L[7].mapPartitions(count_in_a_partition).collect())\n",
    "print(L[8].mapPartitions(count_in_a_partition).collect())\n",
    "print(L[9].mapPartitions(count_in_a_partition).collect())\n",
    "print(L[0].mapPartitions(count_in_a_partition).collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(L[0].mapPartitions(count_in_a_partition).collect())\n",
    "a = L[0].repartition(4)\n",
    "print(a.mapPartitions(count_in_a_partition).collect())\n",
    "b = a.repartition(4)\n",
    "print(a.mapPartitions(count_in_a_partition).collect())\n",
    "c = b.repartition(4)\n",
    "print(a.mapPartitions(count_in_a_partition).collect())\n",
    "d = c.repartition(4)\n",
    "print(a.mapPartitions(count_in_a_partition).collect())\n",
    "e = d.repartition(4)\n",
    "print(a.mapPartitions(count_in_a_partition).collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BigL = L[0].map(lambda x: x[0])\n",
    "print(BigL.mapPartitions(count_in_a_partition).collect())\n",
    "f = BigL.repartition(4)\n",
    "print(f.mapPartitions(count_in_a_partition).collect())\n",
    "\n",
    "BigL2 = L[1].map(lambda x: x[0])\n",
    "print(BigL2.mapPartitions(count_in_a_partition).collect())\n",
    "g = BigL2.repartition(4)\n",
    "print(g.mapPartitions(count_in_a_partition).collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Statistics.corr(f,g)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BigL = L[0].map(lambda x: x[0]).repartition(4)\n",
    "print(BigL.mapPartitions(count_in_a_partition).collect())\n",
    "f = BigL.repartition(4)\n",
    "print(f.mapPartitions(count_in_a_partition).collect())\n",
    "\n",
    "BigL2 = L[1].map(lambda x: x[0]).repartition(4)\n",
    "print(BigL2.mapPartitions(count_in_a_partition).collect())\n",
    "g  = BigL2.repartition(4)\n",
    "print(g.mapPartitions(count_in_a_partition).collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Statistics.corr(f,g)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "v = Vectors.dense(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.mllib.linalg import Vector, Vectors\n",
    "import numpy as np\n",
    "\n",
    "Z = []\n",
    "for i in range(len(L)):\n",
    "    s = np.array(L[i].map(lambda x: x[0]).collect())\n",
    "    Z.append(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TestRDD = sc.parallelize(Z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_in_a_partition(iterator):\n",
    "  yield sum(1 for _ in iterator)\n",
    "\n",
    "TestRDD\n",
    "print(TestRDD.mapPartitions(count_in_a_partition).collect())\n",
    "    \n",
    "pdDF = pd.DataFrame(Z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pdDF.to_csv(\"./fooo.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Z[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = np.array(L[2].map(lambda x: x[0]).collect())\n",
    "s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.array([1.0, 10.0, 100.0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.mllib.linalg import Vector, Vectors\n",
    "import numpy as np\n",
    "from pyspark.mllib.stat import Statistics\n",
    "import pandas as pd\n",
    "from hdfs import Config\n",
    "from pyspark.mllib.stat import Statistics\n",
    "import time\n",
    "\n",
    "\n",
    "######## CONFIG #######################\n",
    "SOURCE_DIR = \"hdfs://ad-cluster-master:54310/user/cluster/source_wfile/\"\n",
    "num_cores = 16\n",
    "\n",
    "\n",
    "#######################################\n",
    "t1 = time.time()\n",
    "client = Config().get_client()\n",
    "FilesList = client.list('/user/cluster/source_wfile')\n",
    "#FilesList = FilesList[:10] #Debug\n",
    "total = len(FilesList)\n",
    "\n",
    "L=[]\n",
    "i=0\n",
    "for file_ in FilesList:\n",
    "    i=i+1\n",
    "    print(\"#########\")\n",
    "    print(\"Work: \" + str(i) + \"/\" + str(total))\n",
    "\n",
    "    dftemp = spark.read.format(\"csv\")\\\n",
    "            .option(\"header\", \"true\")\\\n",
    "            .option(\"mode\", \"PERMISSIVE\")\\\n",
    "            .option(\"inferSchema\", \"true\")\\\n",
    "            .option(\"delimiter\", \",\")\\\n",
    "            .csv(SOURCE_DIR + file_)\n",
    "    \n",
    "    rddT = dftemp.rdd\n",
    "    #print(rddT)\n",
    "    #rddT = dftemp.rdd.map(lambda x: x[0])\n",
    "    #rddT = rddT.repartition(1)\n",
    "    rddT = rddT.repartition(4*num_cores)\n",
    "    #rddT.persist(StorageLevel.MEMORY_AND_DISK)\n",
    "    L.append(rddT)\n",
    "    \n",
    "    \n",
    "    '''dftemp.repartition(3*num_cores)\n",
    "    dftemp.persist(StorageLevel.MEMORY_AND_DISK)\n",
    "    P.append(dftemp)'''\n",
    "t2 = time.time()\n",
    "print(\"####################################\")\n",
    "print(\"TIME TAKEN FOR TASK:\" + str(t2-t1))\n",
    "\n",
    "t1 = time.time()\n",
    "print(\"#############################\")\n",
    "print(\"FILE LOADING TASK FINISHED\")\n",
    "print(\"\\n\")\n",
    "\n",
    "Z = []\n",
    "for i in range(len(L)):\n",
    "    s = np.array(L[i].map(lambda x: x[0]).collect())\n",
    "    Z.append(s)\n",
    "    \n",
    "t2 = time.time()\n",
    "print(\"####################################\")\n",
    "print(\"TIME TAKEN FOR TASK:\" + str(t2-t1))\n",
    "\n",
    "t1 = time.time()\n",
    "print(\"#############################\")\n",
    "print(\"COLLECT TASK FINISHED\")\n",
    "print(\"\\n\")\n",
    "\n",
    "M = np.matrix(Z)\n",
    "N = M.transpose()\n",
    "\n",
    "t2 = time.time()\n",
    "print(\"####################################\")\n",
    "print(\"TIME TAKEN FOR TASK:\" + str(t2-t1))\n",
    "\n",
    "t1 = time.time()\n",
    "print(\"#############################\")\n",
    "print(\"PIPE NUMPY TASK FINISHED\")\n",
    "print(\"\\n\")\n",
    "\n",
    "NewRDD = sc.parallelize(np.asarray(N))\n",
    "A = Statistics.corr(NewRDD)\n",
    "A\n",
    "\n",
    "t2 = time.time()\n",
    "print(\"####################################\")\n",
    "print(\"TIME TAKEN FOR TASK:\" + str(t2-t1))\n",
    "print(\"JOB FINISHED\")\n",
    "print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.mllib.linalg import Vector, Vectors\n",
    "import numpy as np\n",
    "from pyspark.mllib.stat import Statistics\n",
    "import pandas as pd\n",
    "from hdfs import Config\n",
    "from pyspark.mllib.stat import Statistics\n",
    "import time\n",
    "\n",
    "\n",
    "######## CONFIG #######################\n",
    "SOURCE_DIR = \"hdfs://ad-cluster-master:54310/user/cluster/source_wfile/\"\n",
    "num_cores = 16\n",
    "\n",
    "\n",
    "#######################################\n",
    "t1 = time.time()\n",
    "client = Config().get_client()\n",
    "FilesList = client.list('/user/cluster/source_wfile')\n",
    "FilesList = FilesList[:10] #Debug\n",
    "total = len(FilesList)\n",
    "\n",
    "L=[]\n",
    "i=0\n",
    "for file_ in FilesList:\n",
    "    i=i+1\n",
    "    print(\"#########\")\n",
    "    print(\"Work: \" + str(i) + \"/\" + str(total))\n",
    "\n",
    "    dftemp = spark.read.format(\"csv\")\\\n",
    "            .option(\"header\", \"true\")\\\n",
    "            .option(\"mode\", \"PERMISSIVE\")\\\n",
    "            .option(\"inferSchema\", \"true\")\\\n",
    "            .option(\"delimiter\", \",\")\\\n",
    "            .csv(SOURCE_DIR + file_)\n",
    "    \n",
    "    rddT = dftemp.rdd\n",
    "    #print(rddT)\n",
    "    #rddT = dftemp.rdd.map(lambda x: x[0])\n",
    "    #rddT = rddT.repartition(1)\n",
    "    #rddT = rddT.repartition(4*num_cores)\n",
    "    #rddT.persist(StorageLevel.MEMORY_AND_DISK)\n",
    "    L.append(rddT)\n",
    "    \n",
    "    \n",
    "    '''dftemp.repartition(3*num_cores)\n",
    "    dftemp.persist(StorageLevel.MEMORY_AND_DISK)\n",
    "    P.append(dftemp)'''\n",
    "t2 = time.time()\n",
    "print(\"####################################\")\n",
    "print(\"TIME TAKEN FOR TASK:\" + str(t2-t1))\n",
    "\n",
    "L\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t1 = time.time()\n",
    "print(\"#############################\")\n",
    "print(\"FILE LOADING TASK FINISHED\")\n",
    "print(\"\\n\")\n",
    "\n",
    "Z = np.empty(len(L),dtype=object)\n",
    "\n",
    "for i in range(len(L)):\n",
    "    print(i)\n",
    "    a = L[i].map(lambda x: x[0]).collect()\n",
    "    print(type(a))\n",
    "    break\n",
    "    Z[i] = np.array(a)\n",
    "    \n",
    "t2 = time.time()\n",
    "print(\"####################################\")\n",
    "print(\"TIME TAKEN FOR TASK:\" + str(t2-t1))\n",
    "print(\"#############################\")\n",
    "print(\"COLLECT TASK FINISHED\")\n",
    "print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t1 = time.time()\n",
    "print(len(Z))\n",
    "Z = Z.transpose()\n",
    "print(len(Z))\n",
    "\n",
    "t2 = time.time()\n",
    "print(\"####################################\")\n",
    "print(\"TIME TAKEN FOR TASK:\" + str(t2-t1))\n",
    "print(\"#############################\")\n",
    "print(\"PIPE NUMPY TASK FINISHED\")\n",
    "print(\"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.matrix(Z)\n",
    "np.matrix(Z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t1 = time.time()\n",
    "\n",
    "NewRDD = sc.parallelize(Z,4*20)\n",
    "#A = Statistics.corr(NewRDD)\n",
    "\n",
    "t2 = time.time()\n",
    "print(\"####################################\")\n",
    "print(\"TIME TAKEN FOR TASK:\" + str(t2-t1))\n",
    "print(\"JOB FINISHED\")\n",
    "print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t1 = time.time()\n",
    "\n",
    "A = Statistics.corr(NewRDD)\n",
    "\n",
    "t2 = time.time()\n",
    "print(\"####################################\")\n",
    "print(\"TIME TAKEN FOR TASK:\" + str(t2-t1))\n",
    "print(\"JOB FINISHED\")\n",
    "print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t1 = time.time()\n",
    "\n",
    "M = np.matrix(Z[:50])\n",
    "N = M.transpose()\n",
    "\n",
    "t2 = time.time()\n",
    "print(\"####################################\")\n",
    "print(\"TIME TAKEN FOR TASK:\" + str(t2-t1))\n",
    "print(\"#############################\")\n",
    "print(\"PIPE NUMPY TASK FINISHED\")\n",
    "print(\"\\n\")\n",
    "\n",
    "t1 = time.time()\n",
    "\n",
    "NewRDD = sc.parallelize(np.asarray(N),4*num_cores)\n",
    "A = Statistics.corr(NewRDD)\n",
    "\n",
    "t2 = time.time()\n",
    "print(\"####################################\")\n",
    "print(\"TIME TAKEN FOR TASK:\" + str(t2-t1))\n",
    "print(\"JOB FINISHED\")\n",
    "print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.savetxt(\"./supfoot.csv\",A,delimiter=\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.savetxt(\"./supsupfoot.csv\",N,delimiter=\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ne"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t1 = time.time()\n",
    "print(\"#############################\")\n",
    "print(\"FILE LOADING TASK FINISHED\")\n",
    "print(\"\\n\")\n",
    "\n",
    "Z = []\n",
    "\n",
    "for i in range(len(L)):\n",
    "    Z.append(L[i].map(lambda x: x[0]))\n",
    "\n",
    "\n",
    "t2 = time.time()\n",
    "print(\"####################################\")\n",
    "print(\"TIME TAKEN FOR TASK:\" + str(t2-t1))\n",
    "print(\"#############################\")\n",
    "print(\"COLLECT TASK FINISHED\")\n",
    "print(\"\\n\")\n",
    "Z\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A = L[0].map(lambda x: np.asarray(x))\n",
    "A.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rddTranspose(rdd):\n",
    "    rddT1 = rdd.zipWithIndex().flatMap(lambda (x,i): [(i,j,e) for (j,e) in enumerate(x)])\n",
    "    rddT2 = rddT1.map(lambda (i,j,e): (j, (i,e))).groupByKey().sortByKey()\n",
    "    rddT3 = rddT2.map(lambda (i, x): sorted(list(x), cmp=lambda (i1,e1),(i2,e2) : cmp(i1, i2)))\n",
    "    rddT4 = rddT3.map(lambda x: map(lambda (i, y): y , x))\n",
    "    return rddT4\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "R4 = rddTranspose(L[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "K = []\n",
    "for i in range(len(L)):\n",
    "    print(i)\n",
    "    K.append(rddTranspose(L[i]))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "C = O.map(lambda x: np.asarray(x))\n",
    "E = C.collect()\n",
    "E"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "P = sc.union(L[:2])\n",
    "P.persist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "D = P.map(lambda x: np.asarray(x))\n",
    "F = D.collect()\n",
    "F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "O = sc.union(K)\n",
    "O.persist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Q = rddTranspose(O)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "R = Q.map(lambda x: np.asarray(x))\n",
    "G = R.collect()\n",
    "G"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Out = Statistics.corr(Q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tex = np.asarray(G)\n",
    "\n",
    "np.savetxt(\"./Outfoo.csv\", tex, delimiter=\",\")\n",
    "\n",
    "np.savetxt(\"./OutCorr.csv\", Out, delimiter=\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.mllib.linalg import Vector, Vectors\n",
    "import numpy as np\n",
    "from pyspark.mllib.stat import Statistics\n",
    "import pandas as pd\n",
    "from hdfs import Config\n",
    "from pyspark.mllib.stat import Statistics\n",
    "import time\n",
    "\n",
    "\n",
    "######## CONFIG #######################\n",
    "SOURCE_DIR = \"hdfs://ad-cluster-master:54310/user/cluster/source_wfile/\"\n",
    "num_cores = 16\n",
    "\n",
    "\n",
    "#########################################################################\n",
    "t1 = time.time()\n",
    "client = Config().get_client()\n",
    "FilesList = client.list('/user/cluster/source_wfile')\n",
    "#FilesList = FilesList[:10] #Debug\n",
    "total = len(FilesList)\n",
    "\n",
    "L=[]\n",
    "i=0\n",
    "for file_ in FilesList:\n",
    "    i=i+1\n",
    "    print(\"#########\")\n",
    "    print(\"Work: \" + str(i) + \"/\" + str(total))\n",
    "\n",
    "    dftemp = spark.read.format(\"csv\")\\\n",
    "            .option(\"header\", \"true\")\\\n",
    "            .option(\"mode\", \"PERMISSIVE\")\\\n",
    "            .option(\"inferSchema\", \"true\")\\\n",
    "            .option(\"delimiter\", \",\")\\\n",
    "            .csv(SOURCE_DIR + file_)\n",
    "    \n",
    "    rddT = dftemp.rdd.repartition(4*num_cores)\n",
    "    #print(rddT)\n",
    "    #rddT = dftemp.rdd.map(lambda x: x[0])\n",
    "    #rddT = rddT.repartition(1)\n",
    "    #rddT = rddT.repartition(4*num_cores)\n",
    "    #rddT.persist(StorageLevel.MEMORY_AND_DISK)\n",
    "    L.append(rddT)\n",
    "    \n",
    "    \n",
    "    '''dftemp.repartition(3*num_cores)\n",
    "    dftemp.persist(StorageLevel.MEMORY_AND_DISK)\n",
    "    P.append(dftemp)'''\n",
    "    \n",
    "t2 = time.time()\n",
    "print(\"####################################\")\n",
    "print(\"TIME TAKEN FOR TASK:\" + str(t2-t1))\n",
    "\n",
    "print(\"#############################\")\n",
    "print(\"FILE LOADING TASK FINISHED\")\n",
    "print(\"\\n\")\n",
    "\n",
    "####################################################################\n",
    "def rddTranspose(rdd):\n",
    "    rddT1 = rdd.zipWithIndex().flatMap(lambda (x,i): [(i,j,e) for (j,e) in enumerate(x)])\n",
    "    rddT2 = rddT1.map(lambda (i,j,e): (j, (i,e))).groupByKey().sortByKey()\n",
    "    rddT3 = rddT2.map(lambda (i, x): sorted(list(x), cmp=lambda (i1,e1),(i2,e2) : cmp(i1, i2)))\n",
    "    rddT4 = rddT3.map(lambda x: map(lambda (i, y): y , x))\n",
    "    return rddT4\n",
    "\n",
    "\n",
    "t1 = time.time()\n",
    "\n",
    "K = []\n",
    "total = len(L)\n",
    "for i in range(total):\n",
    "    print(\"Transpose: \" + str(i) + \"/\" + str(total))\n",
    "    K.append(rddTranspose(L[i]))\n",
    "    \n",
    "t2 = time.time()\n",
    "print(\"####################################\")\n",
    "print(\"TIME TAKEN FOR TASK:\" + str(t2-t1))\n",
    "\n",
    "print(\"#############################\")\n",
    "print(\"FIRST TRANSPOSE TASK FINISHED\")\n",
    "print(\"\\n\")\n",
    "\n",
    "####################################################################\n",
    "t1 = time.time()\n",
    "\n",
    "\n",
    "O = sc.union(K)\n",
    "print(\"UNION OVER\")\n",
    "Q = rddTranspose(O)\n",
    "\n",
    "t2 = time.time()\n",
    "print(\"####################################\")\n",
    "print(\"TIME TAKEN FOR TASK:\" + str(t2-t1))\n",
    "\n",
    "print(\"#############################\")\n",
    "print(\"UNION AND SECOND TRANSPOSE TASK FINISHED\")\n",
    "print(\"\\n\")\n",
    "\n",
    "'''########################## DEBUG #########################################\n",
    "t1 = time.time()\n",
    "R = Q.map(lambda x: np.asarray(x))\n",
    "R.persist(StorageLevel.MEMORY_AND_DISK)\n",
    "G = R.collect()\n",
    "\n",
    "t2 = time.time()\n",
    "print(\"####################################\")\n",
    "print(\"TIME TAKEN FOR TASK:\" + str(t2-t1))\n",
    "\n",
    "print(\"#############################\")\n",
    "print(\"SECOND TRANSPOSE TASK FINISHED\")\n",
    "print(\"\\n\")\n",
    "'''\n",
    "\n",
    "####################################################################\n",
    "t1 = time.time()\n",
    "Out = Statistics.corr(Q)\n",
    "t2 = time.time()\n",
    "\n",
    "print(\"####################################\")\n",
    "print(\"TIME TAKEN FOR TASK:\" + str(t2-t1))\n",
    "\n",
    "print(\"#############################\")\n",
    "print(\"CORRELATION COMPUTATION FINISHED\")\n",
    "print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rddTranspose(rdd):\n",
    "    rddT1 = rdd.zipWithIndex().flatMap(lambda (x,i): [(i,j,e) for (j,e) in enumerate(x)])\n",
    "    rddT2 = rddT1.map(lambda (i,j,e): (j, (i,e))).groupByKey().sortByKey()\n",
    "    rddT3 = rddT2.map(lambda (i, x): sorted(list(x), cmp=lambda (i1,e1),(i2,e2) : cmp(i1, i2)))\n",
    "    rddT4 = rddT3.map(lambda x: map(lambda (i, y): y , x))\n",
    "    return rddT4\n",
    "\n",
    "\n",
    "t1 = time.time()\n",
    "\n",
    "K = []\n",
    "total = len(L)\n",
    "for i in range(total):\n",
    "    print(\"Transpose: \" + str(i+1) + \"/\" + str(total))\n",
    "    K.append(rddTranspose(L[i]))\n",
    "    \n",
    "t2 = time.time()\n",
    "print(\"####################################\")\n",
    "print(\"TIME TAKEN FOR TASK:\" + str(t2-t1))\n",
    "\n",
    "print(\"#############################\")\n",
    "print(\"FIRST TRANSPOSE TASK FINISHED\")\n",
    "print(\"\\n\")\n",
    "\n",
    "####################################################################\n",
    "t1 = time.time()\n",
    "\n",
    "\n",
    "O = sc.union(K)\n",
    "print(\"UNION OVER\")\n",
    "Q = rddTranspose(O)\n",
    "\n",
    "t2 = time.time()\n",
    "print(\"####################################\")\n",
    "print(\"TIME TAKEN FOR TASK:\" + str(t2-t1))\n",
    "\n",
    "print(\"#############################\")\n",
    "print(\"UNION AND SECOND TRANSPOSE TASK FINISHED\")\n",
    "print(\"\\n\")\n",
    "\n",
    "'''########################## DEBUG #########################################\n",
    "t1 = time.time()\n",
    "R = Q.map(lambda x: np.asarray(x))\n",
    "R.persist(StorageLevel.MEMORY_AND_DISK)\n",
    "G = R.collect()\n",
    "\n",
    "t2 = time.time()\n",
    "print(\"####################################\")\n",
    "print(\"TIME TAKEN FOR TASK:\" + str(t2-t1))\n",
    "\n",
    "print(\"#############################\")\n",
    "print(\"SECOND TRANSPOSE TASK FINISHED\")\n",
    "print(\"\\n\")\n",
    "'''\n",
    "\n",
    "####################################################################\n",
    "t1 = time.time()\n",
    "Out = Statistics.corr(Q)\n",
    "t2 = time.time()\n",
    "\n",
    "print(\"####################################\")\n",
    "print(\"TIME TAKEN FOR TASK:\" + str(t2-t1))\n",
    "\n",
    "print(\"#############################\")\n",
    "print(\"CORRELATION COMPUTATION FINISHED\")\n",
    "print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import islice\n",
    "\n",
    "rddFile = sc.textFile(SOURCE_DIR + FilesList[0],4*num_cores)\n",
    "\n",
    "PL = rddFile.mapPartitionsWithIndex(lambda i, iter: islice(iter, 1, None) if i == 0 else iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PL.getNumPartitions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.mllib.linalg import Vector, Vectors\n",
    "import numpy as np\n",
    "from pyspark.mllib.stat import Statistics\n",
    "import pandas as pd\n",
    "from hdfs import Config\n",
    "from pyspark.mllib.stat import Statistics\n",
    "import time\n",
    "from itertools import islice\n",
    "\n",
    "\n",
    "######## CONFIG #######################\n",
    "SOURCE_DIR = \"hdfs://ad-cluster-master:54310/user/cluster/source_wfile/\"\n",
    "num_cores = 16\n",
    "\n",
    "\n",
    "#########################################################################\n",
    "t1 = time.time()\n",
    "client = Config().get_client()\n",
    "FilesList = client.list('/user/cluster/source_wfile')\n",
    "#FilesList = FilesList[:10] #Debug\n",
    "total = len(FilesList)\n",
    "\n",
    "L=[]\n",
    "i=0\n",
    "for file_ in FilesList:\n",
    "    i=i+1\n",
    "    print(\"#########\")\n",
    "    print(\"Work: \" + str(i) + \"/\" + str(total))\n",
    "\n",
    "    rddFile = sc.textFile(SOURCE_DIR + file_,4*num_cores)\n",
    "    \n",
    "    rddT = rddFile.mapPartitionsWithIndex(lambda i, iter: islice(iter, 1, None) if i == 0 else iter)\n",
    "    #print(rddT)\n",
    "    #rddT = dftemp.rdd.map(lambda x: x[0])\n",
    "    #rddT = rddT.repartition(1)\n",
    "    #rddT = rddT.repartition(4*num_cores)\n",
    "    #rddT.persist(StorageLevel.MEMORY_AND_DISK)\n",
    "    L.append(rddT)\n",
    "    \n",
    "    \n",
    "    '''dftemp.repartition(3*num_cores)\n",
    "    dftemp.persist(StorageLevel.MEMORY_AND_DISK)\n",
    "    P.append(dftemp)'''\n",
    "    \n",
    "t2 = time.time()\n",
    "print(\"####################################\")\n",
    "print(\"TIME TAKEN FOR TASK:\" + str(t2-t1))\n",
    "\n",
    "print(\"#############################\")\n",
    "print(\"FILE LOADING TASK FINISHED\")\n",
    "print(\"\\n\")\n",
    "\n",
    "####################################################################\n",
    "def rddTranspose(rdd):\n",
    "    rddT1 = rdd.zipWithIndex().flatMap(lambda (x,i): [(i,j,e) for (j,e) in enumerate(x)])\n",
    "    rddT2 = rddT1.map(lambda (i,j,e): (j, (i,e))).groupByKey().sortByKey()\n",
    "    rddT3 = rddT2.map(lambda (i, x): sorted(list(x), cmp=lambda (i1,e1),(i2,e2) : cmp(i1, i2)))\n",
    "    rddT4 = rddT3.map(lambda x: map(lambda (i, y): y , x))\n",
    "    return rddT4\n",
    "\n",
    "\n",
    "t1 = time.time()\n",
    "\n",
    "K = []\n",
    "total = len(L)\n",
    "for i in range(total):\n",
    "    print(\"Transpose: \" + str(i+1) + \"/\" + str(total))\n",
    "    K.append(rddTranspose(L[i]))\n",
    "    \n",
    "t2 = time.time()\n",
    "print(\"####################################\")\n",
    "print(\"TIME TAKEN FOR TASK:\" + str(t2-t1))\n",
    "\n",
    "print(\"#############################\")\n",
    "print(\"FIRST TRANSPOSE TASK FINISHED\")\n",
    "print(\"\\n\")\n",
    "\n",
    "####################################################################\n",
    "t1 = time.time()\n",
    "\n",
    "\n",
    "O = sc.union(K)\n",
    "print(\"UNION OVER\")\n",
    "Q = rddTranspose(O)\n",
    "\n",
    "t2 = time.time()\n",
    "print(\"####################################\")\n",
    "print(\"TIME TAKEN FOR TASK:\" + str(t2-t1))\n",
    "\n",
    "print(\"#############################\")\n",
    "print(\"UNION AND SECOND TRANSPOSE TASK FINISHED\")\n",
    "print(\"\\n\")\n",
    "\n",
    "'''########################## DEBUG #########################################\n",
    "t1 = time.time()\n",
    "R = Q.map(lambda x: np.asarray(x))\n",
    "R.persist(StorageLevel.MEMORY_AND_DISK)\n",
    "G = R.collect()\n",
    "\n",
    "t2 = time.time()\n",
    "print(\"####################################\")\n",
    "print(\"TIME TAKEN FOR TASK:\" + str(t2-t1))\n",
    "\n",
    "print(\"#############################\")\n",
    "print(\"SECOND TRANSPOSE TASK FINISHED\")\n",
    "print(\"\\n\")\n",
    "'''\n",
    "\n",
    "####################################################################\n",
    "t1 = time.time()\n",
    "Out = Statistics.corr(Q)\n",
    "t2 = time.time()\n",
    "\n",
    "print(\"####################################\")\n",
    "print(\"TIME TAKEN FOR TASK:\" + str(t2-t1))\n",
    "\n",
    "print(\"#############################\")\n",
    "print(\"CORRELATION COMPUTATION FINISHED\")\n",
    "print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.mllib.linalg import Vector, Vectors\n",
    "import numpy as np\n",
    "from pyspark.mllib.stat import Statistics\n",
    "import pandas as pd\n",
    "from hdfs import Config\n",
    "from pyspark.mllib.stat import Statistics\n",
    "import time\n",
    "from itertools import islice\n",
    "\n",
    "\n",
    "######## CONFIG #######################\n",
    "SOURCE_DIR = \"hdfs://ad-cluster-master:54310/user/cluster/workfileBis/\"\n",
    "num_cores = 16\n",
    "\n",
    "\n",
    "#########################################################################\n",
    "t1 = time.time()\n",
    "client = Config().get_client()\n",
    "FilesList = client.list('/user/cluster/workfileBis')\n",
    "#FilesList = FilesList[:10] #Debug\n",
    "total = len(FilesList)\n",
    "\n",
    "L=[]\n",
    "i=0\n",
    "for file_ in FilesList:\n",
    "    i=i+1\n",
    "    print(\"#########\")\n",
    "    print(\"Work: \" + str(i) + \"/\" + str(total))\n",
    "\n",
    "    rddFile = sc.textFile(SOURCE_DIR + file_,4*num_cores)\n",
    "    rddT = rddFile\n",
    "    #rddT = rddFile.mapPartitionsWithIndex(lambda i, iter: islice(iter, 1, None) if i == 0 else iter)\n",
    "    #print(rddT)\n",
    "    #rddT = dftemp.rdd.map(lambda x: x[0])\n",
    "    #rddT = rddT.repartition(1)\n",
    "    #rddT = rddT.repartition(4*num_cores)\n",
    "    #rddT.persist(StorageLevel.MEMORY_AND_DISK)\n",
    "    L.append(rddT)\n",
    "    \n",
    "    \n",
    "    '''dftemp.repartition(3*num_cores)\n",
    "    dftemp.persist(StorageLevel.MEMORY_AND_DISK)\n",
    "    P.append(dftemp)'''\n",
    "    \n",
    "t2 = time.time()\n",
    "print(\"####################################\")\n",
    "print(\"TIME TAKEN FOR TASK:\" + str(t2-t1))\n",
    "\n",
    "print(\"#############################\")\n",
    "print(\"FILE LOADING TASK FINISHED\")\n",
    "print(\"\\n\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "L[4].map(lambda x: np.asarray(x)).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "O = sc.union(L)\n",
    "print(\"UNION OVER\")\n",
    "O"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "####################################################################\n",
    "def rddTranspose(rdd):\n",
    "    rddT1 = rdd.zipWithIndex().flatMap(lambda (x,i): [(i,j,e) for (j,e) in enumerate(x)])\n",
    "    rddT2 = rddT1.map(lambda (i,j,e): (j, (i,e))).groupByKey().sortByKey()\n",
    "    rddT3 = rddT2.map(lambda (i, x): sorted(list(x), cmp=lambda (i1,e1),(i2,e2) : cmp(i1, i2)))\n",
    "    rddT4 = rddT3.map(lambda x: map(lambda (i, y): y , x))\n",
    "    return rddT4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t1 = time.time()\n",
    "Q = rddTranspose(O)\n",
    "t2 = time.time()\n",
    "print(\"####################################\")\n",
    "print(\"TIME TAKEN FOR TASK:\" + str(t2-t1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "####################################################################\n",
    "def rddTranspose(rdd):\n",
    "    rddT1 = rdd.zipWithIndex().flatMap(lambda (x,i): [(i,j,e) for (j,e) in enumerate(x)])\n",
    "    rddT2 = rddT1.map(lambda (i,j,e): (j, (i,e))).groupByKey().sortByKey()\n",
    "    rddT3 = rddT2.map(lambda (i, x): sorted(list(x), cmp=lambda (i1,e1),(i2,e2) : cmp(i1, i2)))\n",
    "    rddT4 = rddT3.map(lambda x: map(lambda (i, y): y , x))\n",
    "    return rddT4\n",
    "\n",
    "\n",
    "t1 = time.time()\n",
    "\n",
    "K = []\n",
    "total = len(L)\n",
    "for i in range(total):\n",
    "    print(\"Transpose: \" + str(i+1) + \"/\" + str(total))\n",
    "    K.append(rddTranspose(L[i]))\n",
    "    \n",
    "t2 = time.time()\n",
    "print(\"####################################\")\n",
    "print(\"TIME TAKEN FOR TASK:\" + str(t2-t1))\n",
    "\n",
    "print(\"#############################\")\n",
    "print(\"FIRST TRANSPOSE TASK FINISHED\")\n",
    "print(\"\\n\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "####################################################################\n",
    "t1 = time.time()\n",
    "\n",
    "\n",
    "O = sc.union(K)\n",
    "print(\"UNION OVER\")\n",
    "Q = rddTranspose(O)\n",
    "\n",
    "t2 = time.time()\n",
    "print(\"####################################\")\n",
    "print(\"TIME TAKEN FOR TASK:\" + str(t2-t1))\n",
    "\n",
    "print(\"#############################\")\n",
    "print(\"UNION AND SECOND TRANSPOSE TASK FINISHED\")\n",
    "print(\"\\n\")\n",
    "\n",
    "'''########################## DEBUG #########################################\n",
    "t1 = time.time()\n",
    "R = Q.map(lambda x: np.asarray(x))\n",
    "R.persist(StorageLevel.MEMORY_AND_DISK)\n",
    "G = R.collect()\n",
    "\n",
    "t2 = time.time()\n",
    "print(\"####################################\")\n",
    "print(\"TIME TAKEN FOR TASK:\" + str(t2-t1))\n",
    "\n",
    "print(\"#############################\")\n",
    "print(\"SECOND TRANSPOSE TASK FINISHED\")\n",
    "print(\"\\n\")\n",
    "'''\n",
    "\n",
    "####################################################################\n",
    "t1 = time.time()\n",
    "Out = Statistics.corr(Q)\n",
    "t2 = time.time()\n",
    "\n",
    "print(\"####################################\")\n",
    "print(\"TIME TAKEN FOR TASK:\" + str(t2-t1))\n",
    "\n",
    "print(\"#############################\")\n",
    "print(\"CORRELATION COMPUTATION FINISHED\")\n",
    "print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "O"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "O.persist(StorageLevel.MEMORY_AND_DISK)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.mllib.linalg import Vector, Vectors\n",
    "import numpy as np\n",
    "from pyspark.mllib.stat import Statistics\n",
    "import pandas as pd\n",
    "from hdfs import Config\n",
    "from pyspark.mllib.stat import Statistics\n",
    "import time\n",
    "from itertools import islice\n",
    "\n",
    "\n",
    "######## CONFIG #######################\n",
    "SOURCE_DIR = \"hdfs://ad-cluster-master:54310/user/cluster/workfileBis/\"\n",
    "num_cores = 16\n",
    "\n",
    "\n",
    "#########################################################################\n",
    "t1 = time.time()\n",
    "client = Config().get_client()\n",
    "FilesList = client.list('/user/cluster/workfileBis')\n",
    "FilesList = FilesList[:3] #Debug\n",
    "total = len(FilesList)\n",
    "\n",
    "L=[]\n",
    "i=0\n",
    "for file_ in FilesList:\n",
    "    i=i+1\n",
    "    print(\"#########\")\n",
    "    print(\"Work: \" + str(i) + \"/\" + str(total))\n",
    "\n",
    "    rddFile = sc.textFile(SOURCE_DIR + file_ ,4*num_cores)\n",
    "    rddT = rddFile.map(lambda x: x.split(\",\")[1:])\n",
    "    #rddT = rddFile.mapPartitionsWithIndex(lambda i, iter: islice(iter, 1, None) if i == 0 else iter)\n",
    "    #print(rddT)\n",
    "    #rddT = dftemp.rdd.map(lambda x: x[0])\n",
    "    #rddT = rddT.repartition(1)\n",
    "    #rddT = rddT.repartition(4*num_cores)\n",
    "    #rddT.persist(StorageLevel.MEMORY_AND_DISK)\n",
    "    L.append(rddT)\n",
    "    \n",
    "    \n",
    "    '''dftemp.repartition(3*num_cores)\n",
    "    dftemp.persist(StorageLevel.MEMORY_AND_DISK)\n",
    "    P.append(dftemp)'''\n",
    "    \n",
    "t2 = time.time()\n",
    "print(\"####################################\")\n",
    "print(\"TIME TAKEN FOR TASK:\" + str(t2-t1))\n",
    "\n",
    "print(\"#############################\")\n",
    "print(\"FILE LOADING TASK FINISHED\")\n",
    "print(\"\\n\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from pyspark.mllib.linalg import Vectors\n",
    "\n",
    "rddFile = sc.textFile(SOURCE_DIR + FilesList[0],4*num_cores)\n",
    "rddT = rddFile.map(lambda x: x.split(\",\")[1:])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "####################################################################\n",
    "def rddTranspose(rdd):\n",
    "    rddT1 = rdd.zipWithIndex().flatMap(lambda (x,i): [(i,j,e) for (j,e) in enumerate(x)])\n",
    "    rddT2 = rddT1.map(lambda (i,j,e): (j, (i,e))).groupByKey().sortByKey()\n",
    "    rddT3 = rddT2.map(lambda (i, x): sorted(list(x), cmp=lambda (i1,e1),(i2,e2) : cmp(i1, i2)))\n",
    "    rddT4 = rddT3.map(lambda x: map(lambda (i, y): y , x))\n",
    "    return rddT4\n",
    "\n",
    "rddTtr = rddTranspose(rddT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DF = rddT.toDF()\n",
    "DF.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DF2 = rddTtr.toDF()\n",
    "DF2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from pyspark.mllib.linalg import Vectors\n",
    "\n",
    "\n",
    "sv1 = Vectors.sparse(3, [0, 2], [1.0, 3.0])\n",
    "\n",
    "sv1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "U = sc.union(L)\n",
    "U\n",
    "\n",
    "DfU = U.toDF()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DfU.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.mllib.linalg import Vector, Vectors\n",
    "import numpy as np\n",
    "from pyspark.mllib.stat import Statistics\n",
    "import pandas as pd\n",
    "from hdfs import Config\n",
    "from pyspark.mllib.stat import Statistics\n",
    "import time\n",
    "from itertools import islice\n",
    "\n",
    "\n",
    "######## CONFIG #######################\n",
    "SOURCE_DIR = \"hdfs://ad-cluster-master:54310/user/cluster/sample/\"\n",
    "num_cores = 16\n",
    "\n",
    "\n",
    "#########################################################################\n",
    "t1 = time.time()\n",
    "client = Config().get_client()\n",
    "FilesList = client.list('/user/cluster/sample')\n",
    "FilesList = FilesList[:3] #Debug\n",
    "total = len(FilesList)\n",
    "\n",
    "L=[]\n",
    "i=0\n",
    "for file_ in FilesList:\n",
    "    i=i+1\n",
    "    print(\"#########\")\n",
    "    print(\"Work: \" + str(i) + \"/\" + str(total))\n",
    "\n",
    "    rddFile = sc.textFile(SOURCE_DIR + file_ ,4*num_cores)\n",
    "    #rddT = rddFile.map(lambda x: x.split(\",\",1)[1])\n",
    "    rddT = rddFile.flatMap(lambda x: (x.split(\",\")[0],\\\n",
    "                                    map(float,x.split(\",\")[1:])\\\n",
    "                                  ))\n",
    "    #rddT = rddFile.map(lambda x: (x.split(\",\")[0],x.split(\",\")[1:]))\n",
    "    #rddT = rddFile.mapPartitionsWithIndex(lambda i, iter: islice(iter, 1, None) if i == 0 else iter)\n",
    "    #print(rddT)\n",
    "    #rddT = dftemp.rdd.map(lambda x: x[0])\n",
    "    #rddT = rddT.repartition(1)\n",
    "    #rddT = rddT.repartition(4*num_cores)\n",
    "    #rddT.persist(StorageLevel.MEMORY_AND_DISK)\n",
    "    L.append(rddT)\n",
    "    \n",
    "    \n",
    "    '''dftemp.repartition(3*num_cores)\n",
    "    dftemp.persist(StorageLevel.MEMORY_AND_DISK)\n",
    "    P.append(dftemp)'''\n",
    "    \n",
    "t2 = time.time()\n",
    "print(\"####################################\")\n",
    "print(\"TIME TAKEN FOR TASK:\" + str(t2-t1))\n",
    "\n",
    "print(\"#############################\")\n",
    "print(\"FILE LOADING TASK FINISHED\")\n",
    "print(\"\\n\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(L)):\n",
    "    print(L[i].collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(L)):\n",
    "    print(L[i].collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for i in range(len(L)):\n",
    "    L[i].map(lambda x: np.asarray(x))\n",
    "    print(L[i].collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "####################################################################\n",
    "def rddTranspose(rdd):\n",
    "    rddT1 = rdd.zipWithIndex().flatMap(lambda (x,i): [(i,j,e) for (j,e) in enumerate(x)])\n",
    "    rddT2 = rddT1.map(lambda (i,j,e): (j, (i,e))).groupByKey().sortByKey()\n",
    "    rddT3 = rddT2.map(lambda (i, x): sorted(list(x), cmp=lambda (i1,e1),(i2,e2) : cmp(i1, i2)))\n",
    "    rddT4 = rddT3.map(lambda x: map(lambda (i, y): y , x))\n",
    "    return rddT4\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "U = sc.union(L)\n",
    "U.collect()\n",
    "\n",
    "#rddTranspose(U).collect()\n",
    "#DfU = U.toDF()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "DfU.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Statistics.corr(U)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "U.map(lambda x: np.asarray(x)).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "V = U.flatMap(lambda l: l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "U.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.mllib.linalg import Vector, Vectors\n",
    "import numpy as np\n",
    "from pyspark.mllib.stat import Statistics\n",
    "import pandas as pd\n",
    "from hdfs import Config\n",
    "from pyspark.mllib.stat import Statistics\n",
    "import time\n",
    "from itertools import islice\n",
    "\n",
    "\n",
    "######## CONFIG #######################\n",
    "SOURCE_DIR = \"hdfs://ad-cluster-master:54310/user/cluster/source_wfile/\"\n",
    "num_cores = 16\n",
    "\n",
    "\n",
    "#########################################################################\n",
    "t1 = time.time()\n",
    "client = Config().get_client()\n",
    "FilesList = client.list('/user/cluster/source_wfile')\n",
    "#FilesList = FilesList[:3] #Debug\n",
    "total = len(FilesList)\n",
    "\n",
    "I=[]\n",
    "i=0\n",
    "for file_ in FilesList:\n",
    "    i=i+1\n",
    "    print(\"#########\")\n",
    "    print(\"Work: \" + str(i) + \"/\" + str(total))\n",
    "\n",
    "    rddFile = sc.textFile(SOURCE_DIR + file_,4*num_cores)\n",
    "    rddT = rddFile.zipWithIndex().map(lambda (x,y): (y,x))\n",
    "    #rddT = rddFile.map(lambda x: x.split(\",\",1)[1])\n",
    "    #rddT = rddFile.flatMap(lambda x: (x.split(\",\")[0],\\\n",
    "    #                                map(float,x.split(\",\")[1:])\\\n",
    "    #                              ))\n",
    "    #rddT = rddFile.map(lambda x: (x.split(\",\")[0],x.split(\",\")[1:]))\n",
    "    #rddT = rddFile.mapPartitionsWithIndex(lambda i, iter: islice(iter, 1, None) if i == 0 else iter)\n",
    "    #print(rddT)\n",
    "    #rddT = dftemp.rdd.map(lambda x: x[0])\n",
    "    #rddT = rddT.repartition(1)\n",
    "    #rddT = rddT.repartition(4*num_cores)\n",
    "    #rddT.persist(StorageLevel.MEMORY_AND_DISK)\n",
    "    I.append(rddT)\n",
    "    \n",
    "    \n",
    "    '''dftemp.repartition(3*num_cores)\n",
    "    dftemp.persist(StorageLevel.MEMORY_AND_DISK)\n",
    "    P.append(dftemp)'''\n",
    "    \n",
    "t2 = time.time()\n",
    "print(\"####################################\")\n",
    "print(\"TIME TAKEN FOR TASK:\" + str(t2-t1))\n",
    "\n",
    "print(\"#############################\")\n",
    "print(\"FILE LOADING TASK FINISHED\")\n",
    "print(\"\\n\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "####################################################################\n",
    "def rddTranspose(rdd):\n",
    "    rddT1 = rdd.zipWithIndex().flatMap(lambda (x,i): [(i,j,e) for (j,e) in enumerate(x)])\n",
    "    rddT2 = rddT1.map(lambda (i,j,e): (j, (i,e))).groupByKey().sortByKey()\n",
    "    rddT3 = rddT2.map(lambda (i, x): sorted(list(x), cmp=lambda (i1,e1),(i2,e2) : cmp(i1, i2)))\n",
    "    rddT4 = rddT3.map(lambda x: map(lambda (i, y): y , x))\n",
    "    return rddT4\n",
    "print(I[0].collect())\n",
    "print(I[1].collect())\n",
    "a = I[0]\n",
    "b = a.join(I[1])\n",
    "print(I[2].collect())\n",
    "c = b.join(I[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "copy\n",
    "enumerate(copy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c.groupByKey().collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "uni_ = sc.union(I)\n",
    "\n",
    "fin = uni_.groupByKey().mapValues(tuple).map(lambda x: x[1])\n",
    "\n",
    "\n",
    "header = list(fin.take(1)[0])\n",
    "\n",
    "#tup1 = index created\n",
    "#tup2 = data\n",
    "#map data to float \n",
    "end = fin.zipWithIndex().filter(lambda tup: tup[1] > 0).map(lambda tup: map(float,tup[0]))\n",
    "Mat = end.toDF(header)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "uni_ = sc.union(I)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fin = uni_.groupByKey().mapValues(tuple).filter(lambda tup: tup[1] > 0).map(lambda x: x[1]).map(lambda tup: map(float,tup[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "header = list(fin.take(1)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(fin.take(1)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "end = fin.zipWithIndex()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Mat = end.toDF(header)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "header\n",
    "Lo = []\n",
    "for i in range(len(FilesList)):\n",
    "    temp = FilesList[i][:-4]\n",
    "    f_c_ = ''.join(e for e in temp if e.isalnum())\n",
    "    Lo.append(f_c_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "header"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Lo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "I[0].collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Mat.stat.corr(str(header[0]),str(header[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for t in enumerate(I):\n",
    "    print t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.mllib.linalg import Vector, Vectors\n",
    "import numpy as np\n",
    "from pyspark.mllib.stat import Statistics\n",
    "import pandas as pd\n",
    "from hdfs import Config\n",
    "from pyspark.mllib.stat import Statistics\n",
    "import time\n",
    "from itertools import islice\n",
    "\n",
    "\n",
    "######## CONFIG #######################\n",
    "SOURCE_DIR = \"hdfs://ad-cluster-master:54310/user/cluster/source_wfile/\"\n",
    "num_cores = 16\n",
    "\n",
    "\n",
    "#########################################################################\n",
    "t1 = time.time()\n",
    "client = Config().get_client()\n",
    "FilesList = client.list('/user/cluster/source_wfile')\n",
    "#FilesList = FilesList[:3] #Debug\n",
    "total = len(FilesList)\n",
    "\n",
    "I=[]\n",
    "i=0\n",
    "header = []\n",
    "for file_ in FilesList:\n",
    "    i=i+1\n",
    "    print(\"#########\")\n",
    "    print(\"Work: \" + str(i) + \"/\" + str(total))\n",
    "\n",
    "    rddFile = sc.textFile(SOURCE_DIR + file_,4*num_cores)\n",
    "    header.append(rddFile.take(1)[0])\n",
    "    \n",
    "    rddT = rddFile.zipWithIndex().map(lambda (x,y): (y,x))\n",
    "    \n",
    "    #rddT = rddFile.map(lambda x: x.split(\",\",1)[1])\n",
    "    #rddT = rddFile.flatMap(lambda x: (x.split(\",\")[0],\\\n",
    "    #                                map(float,x.split(\",\")[1:])\\\n",
    "    #                              ))\n",
    "    #rddT = rddFile.map(lambda x: (x.split(\",\")[0],x.split(\",\")[1:]))\n",
    "    #rddT = rddFile.mapPartitionsWithIndex(lambda i, iter: islice(iter, 1, None) if i == 0 else iter)\n",
    "    #print(rddT)\n",
    "    #rddT = dftemp.rdd.map(lambda x: x[0])\n",
    "    #rddT = rddT.repartition(1)\n",
    "    #rddT = rddT.repartition(4*num_cores)\n",
    "    #rddT.persist(StorageLevel.MEMORY_AND_DISK)\n",
    "    I.append(rddT)\n",
    "    \n",
    "    \n",
    "    '''dftemp.repartition(3*num_cores)\n",
    "    dftemp.persist(StorageLevel.MEMORY_AND_DISK)\n",
    "    P.append(dftemp)'''\n",
    "    \n",
    "t2 = time.time()\n",
    "print(\"####################################\")\n",
    "print(\"TIME TAKEN FOR TASK:\" + str(t2-t1))\n",
    "\n",
    "print(\"#############################\")\n",
    "print(\"FILE LOADING TASK FINISHED\")\n",
    "print(\"\\n\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "uni_ = sc.union(I)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#tup0 = index created\n",
    "#tup1 = data\n",
    "#map data to float \n",
    "fin = uni_.groupByKey().mapValues(tuple)\\\n",
    "                        .filter(lambda tup: tup[0] > 0)\\\n",
    "                        .map(lambda tup: map(float,tup[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = fin.persist(StorageLevel.MEMORY_AND_DISK)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Mat2 = s.toDF(header)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p.take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "Mat2.stat.corr(str(header[0]),str(header[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Array = np.array(Statistics.corr(s))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.mllib.linalg import Vector, Vectors\n",
    "import numpy as np\n",
    "from pyspark.mllib.stat import Statistics\n",
    "import pandas as pd\n",
    "from hdfs import Config\n",
    "from pyspark.mllib.stat import Statistics\n",
    "import time\n",
    "from itertools import islice\n",
    "\n",
    "\n",
    "######## CONFIG #######################\n",
    "SOURCE_DIR = \"hdfs://ad-cluster-master:54310/user/cluster/source_wfile/\"\n",
    "num_cores = 16\n",
    "\n",
    "\n",
    "#########################################################################\n",
    "t1 = time.time()\n",
    "client = Config().get_client()\n",
    "FilesList = client.list('/user/cluster/source_wfile')\n",
    "#FilesList = FilesList[:3] #Debug\n",
    "total = len(FilesList)\n",
    "\n",
    "I=[]\n",
    "i=0\n",
    "header = []\n",
    "for file_ in FilesList:\n",
    "    i=i+1\n",
    "    print(\"#########\")\n",
    "    print(\"Work: \" + str(i) + \"/\" + str(total))\n",
    "\n",
    "    rddFile = sc.textFile(SOURCE_DIR + file_,4*num_cores)\n",
    "    header.append(rddFile.take(1)[0])\n",
    "    \n",
    "    rddT = rddFile.zipWithIndex().filter(lambda col: col[1] > 0).map(lambda x: (map(float,x)[0],x[1]))\n",
    "\n",
    "    df = rddT.toDF([header[i-1],\"uuid\"])\n",
    "    \n",
    "    #rddT = rddFile.map(lambda x: x.split(\",\",1)[1])\n",
    "    #rddT = rddFile.flatMap(lambda x: (x.split(\",\")[0],\\\n",
    "    #                                map(float,x.split(\",\")[1:])\\\n",
    "    #                              ))\n",
    "    #rddT = rddFile.map(lambda x: (x.split(\",\")[0],x.split(\",\")[1:]))\n",
    "    #rddT = rddFile.mapPartitionsWithIndex(lambda i, iter: islice(iter, 1, None) if i == 0 else iter)\n",
    "    #print(rddT)\n",
    "    #rddT = dftemp.rdd.map(lambda x: x[0])\n",
    "    #rddT = rddT.repartition(1)\n",
    "    #rddT = rddT.repartition(4*num_cores)\n",
    "    #rddT.persist(StorageLevel.MEMORY_AND_DISK)\n",
    "    I.append(df)\n",
    "    \n",
    "    \n",
    "    '''dftemp.repartition(3*num_cores)\n",
    "    dftemp.persist(StorageLevel.MEMORY_AND_DISK)\n",
    "    P.append(dftemp)'''\n",
    "    \n",
    "t2 = time.time()\n",
    "print(\"####################################\")\n",
    "print(\"TIME TAKEN FOR TASK:\" + str(t2-t1))\n",
    "\n",
    "print(\"#############################\")\n",
    "print(\"FILE LOADING TASK FINISHED\")\n",
    "print(\"\\n\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "I[325].show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "op = I[0]\n",
    "\n",
    "for i in range(len(FilesList) - 1):\n",
    "    print(\"Task #\" + str(i) + \"/\" + str(len(FilesList)))\n",
    "    op = op.join(I[i+1], \"uuid\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#op.collect()\n",
    "op.stat.corr(str(header[82]), str(header[125]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "I[1].uuid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.mllib.linalg import Vector, Vectors\n",
    "import numpy as np\n",
    "from pyspark.mllib.stat import Statistics\n",
    "import pandas as pd\n",
    "from hdfs import Config\n",
    "from pyspark.mllib.stat import Statistics\n",
    "import time\n",
    "from itertools import islice\n",
    "\n",
    "\n",
    "######## CONFIG #######################\n",
    "SOURCE_DIR = \"hdfs://ad-cluster-master:54310/user/cluster/sample/col/\"\n",
    "num_cores = 16\n",
    "\n",
    "\n",
    "#########################################################################\n",
    "t1 = time.time()\n",
    "client = Config().get_client()\n",
    "FilesList = client.list('/user/cluster/sample/col')\n",
    "#FilesList = FilesList[:3] #Debug\n",
    "total = len(FilesList)\n",
    "\n",
    "I=[]\n",
    "i=0\n",
    "header = []\n",
    "for file_ in FilesList:\n",
    "    i=i+1\n",
    "    print(\"#########\")\n",
    "    print(\"Work: \" + str(i) + \"/\" + str(total))\n",
    "\n",
    "    rddFile = sc.textFile(SOURCE_DIR + file_,4*num_cores)\n",
    "    header.append(rddFile.take(1)[0])\n",
    "    \n",
    "    rddT = rddFile.zipWithIndex().map(lambda (x,y): (y,x))\n",
    "    \n",
    "    #rddT = rddFile.map(lambda x: x.split(\",\",1)[1])\n",
    "    #rddT = rddFile.flatMap(lambda x: (x.split(\",\")[0],\\\n",
    "    #                                map(float,x.split(\",\")[1:])\\\n",
    "    #                              ))\n",
    "    #rddT = rddFile.map(lambda x: (x.split(\",\")[0],x.split(\",\")[1:]))\n",
    "    #rddT = rddFile.mapPartitionsWithIndex(lambda i, iter: islice(iter, 1, None) if i == 0 else iter)\n",
    "    #print(rddT)\n",
    "    #rddT = dftemp.rdd.map(lambda x: x[0])\n",
    "    #rddT = rddT.repartition(1)\n",
    "    #rddT = rddT.repartition(4*num_cores)\n",
    "    #rddT.persist(StorageLevel.MEMORY_AND_DISK)\n",
    "    I.append(rddT)\n",
    "    \n",
    "    \n",
    "    '''dftemp.repartition(3*num_cores)\n",
    "    dftemp.persist(StorageLevel.MEMORY_AND_DISK)\n",
    "    P.append(dftemp)'''\n",
    "    \n",
    "t2 = time.time()\n",
    "print(\"####################################\")\n",
    "print(\"TIME TAKEN FOR TASK:\" + str(t2-t1))\n",
    "\n",
    "print(\"#############################\")\n",
    "print(\"FILE LOADING TASK FINISHED\")\n",
    "print(\"\\n\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "uni_ = sc.union(I)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#tup0 = index created\n",
    "#tup1 = data\n",
    "#map data to float \n",
    "fin = uni_.groupByKey().mapValues(tuple)\\\n",
    "                        .filter(lambda tup: tup[0] > 0)\\\n",
    "                        .map(lambda tup: map(float,tup[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fin.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "spark"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
